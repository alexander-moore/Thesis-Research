# Conclusion {-}

If we don't want Conclusion to have a chapter number next to it, we can add the `{-}` attribute.

## The End
the end of the paper should summarize your main finginds, interpret what your results mean in context, discuss limitations, and point to where there is room for4 improvement or further analaysis. This is your last chance to make an impression, and say everything I want to say.

## Discussion
1. What are the key features of your analysis and results?
2. Do your results confirm or contradict your initial expectations or earlier work?
3. What problems did you run into, and if still unsolved, wgat are suggestions for addressing
4. Knowing what you know now, what reccomendations in context do you have?
5. What should someone work on next, building off of your work?

## Conclusion
A conclusion should not introduce new material. Conclusion brief, and only contain main points of paper. It will be redundant: that is the point. 

## Future Work
There are so many exciting unexplored methods and tweaks that would be worth investigating for the improvement of neural network complex survey imputation methods for mean estimation.

Many I don't know how to implement, couldn't find a good use, don't understand enough, or was computationally restricted, especially during monte carlo simulation!

Can do a paragraph on the most important things here:

- something better than linear regression: local polynomial, etc
- training neural networks correctly: finding the validation minima then re-training every time. I was prevented by computational intensity
- tweaks on neural networks: **reinforcement learning**, dropout layers, noise layers, pre-trained networks
- re-visiting weighted resampling by trying bootstrapping methods, bigger resample. bootstrapping many datasets, training on each, and taking mean of estimates. somethign like this to stabilize predictions and sampling variability.

Multiple imputation: since hopefully NN preserve intervarabhle relationships better than linear or median etc