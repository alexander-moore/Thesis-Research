# Conclusion {-}

If we don't want Conclusion to have a chapter number next to it, we can add the `{-}` attribute.

## Results (SHOULD GO IN OTHER CHAPTERS, 3 AND 4)
Not a place to list findings. Rather, convince that what I did is correct, worthy of study, and impactful. Be honest about the generalizability of the work. If the findings are negative, take care to explain why the results still matter (discussion of why linear regression is so dank at mean estimation). Cast the negative result as a useful insight, rather than a problem with the methodology

## The End
the end of the paper should summarize your main finginds, interpret what your results mean in context, discuss limitations, and point to where there is room for4 improvement or further analaysis. This is your last chance to make an impression, and say everything I want to say.

## Discussion
1. What are the key features of your analysis and results?
2. Do your results confirm or contradict your initial expectations or earlier work?
3. What problems did you run into, and if still unsolved, wgat are suggestions for addressing
4. Knowing what you know now, what reccomendations in context do you have?
5. What should someone work on next, building off of your work?

## Conclusion
A conclusion should not introduce new material. Conclusion brief, and only contain main points of paper. It will be redundant: that is the point. 
## Future Work

There are so many exciting unexplored methods and tweaks that would be worth investigating.

Many I don't know how to implement, couldn't find a good use, don't understand enough, or was computationally restricted, especially during monte carlo simulation!

Can do a paragraph on the most important things here:

- something better than linear regression: local polynomial, etc
- training neural networks correctly: finding the validation minima then re-training every time. I was prevented by computational intensity
- tweaks on neural networks: **reinforcement learning**, dropout layers, noise layers, pre-trained networks
- re-visiting weighted resampling by trying bootstrapping methods, bigger resample. bootstrapping many datasets, training on each, and taking mean of estimates. somethign like this to stabilize predictions and sampling variability.

Multiple imputation: since hopefully NN preserve intervarabhle relationships better than linear or median etc