# Conclusion
## Discussion

Early simulation results showed neural networks struggling to compete with the stability of weighted linear regression mean estimation. The highly nonlinear relationship of the simulation is a product of extensive neural network testing to produce a positive result indicative of potential for neural network capability. The CE results are suprising in this manner: though more variable, neural networks consistently outperform the parametric linear model for outlier prediction, contributing to accuracy and, consequentially, mean estimations.

These results show clear promise to the capability of neural network imputation for systematic missingness in labels with $\pi$ correlation. The significant improvement of each neural network method from weighted linear regression is a positive finding, but one that is slightly undermined due to the total success of neural networks. When all nonlinear methods outperform the linear regression, it is clear that there is a strong nonlinearity between the features and label for the CE data given the selected features.

However, that does not mean these results are unimpactful. The success of the neural network methods is that they thrived in the same minimal-knowledge enviroment that linear regression does. The only change from the simulated neural networks to real-data experiments was a larger network breadth and learning rate, both hyperparameters changed for computational time, not superior accuracy. This is highly promising as the assumptions of NN success were derived from the nonparametric benefits of a more flexible model which is highly adaptable to data relationships. The hypotheses on the advantages of this properly trained family of algorithms are upgrades for mean estimation and imputation accuracy, even when ignoring survey design.

The lower scores of the $\pi$-feature and derived-parameter methods compared to naive neural network imputation is a suprising result. This outcome is at odds with the hypotheses on the feature-selection property outlined in Chapter 2. If the $\pi$ feature and its derivatives were informative to the label, the methods should have superior accuracy, and if the $\pi$ feature was uninformative, the weighted-MSE should have underperformed. The "forced inclusion methods" of weighted MSE and resample would be worth exploring further in expirements where $y$ and $\pi$ are known to be uncorrelated. In this situation, it would be reassuring to see the optional-feature methods succeed in refusing to incorporate information from an uninformative $\pi$, while the forced methods struggled to overcome the disadvantage of additional noise. As in the case of weighted linear regression, the forced methods thrive when the regression is warped to the extrema by outlying $\pi$. In these cases, the algorithm tweaks give extra weight making the often-missing observations make up for their peer's missingness. This counteracts the induced missingness weighted to large labels well.

These results altogether show the benefits of neural networks in minimal domain knowledge imputation: using heuristic feedforward networks and a validation set, the models unilaterally outperformed a typical weighted regression for mean imputation.

If a researcher is interested in an easy to implement, computationally inexpensive heuristic algorithm to impute complex survey data for mean estimation, weighted linear regression is likely the most appealing method. 

Neural network methods are promising, however, because of their many degrees flexibility. The nonparametric models are adaptable to more complex intervariable relationships of the data. If a domain expert hypothesized a sophisticated function on a subset of variables which make an excellent label predictor without knowing its specific form, there is room for that structure in the neural network architecture.

Likewise, the massive space of hyperparameters for even a simple feedforward neural network gives immeasurable potential to a savvy designer. Understanding of the shape of the loss function, necessary load of the model, and relationship of features are all examples of knowledge lending itself to specific selections of model load, activations, optimizers, losses, weightings, connectivity, and countless other tweaks letting a knowledgable designer maximize predictive performance. Weighted linear regression on real data has shown demonstratable inferiorty to the parametric family, and lacks this degree of tuning potential. The results of the CE experiment could diverge even further given algorithmic tweaks optimized to validation data.

## Conclusion

Neural networks are extremely promising in the realm of imputation. Each neural network method using only heuristic default parameters outperformed the weighted linear regression model significantly in the simulation and Consumer Expenditure mean estimation experiments.

Nonparametric modelling with validation data should outperform linear models, even when survey design information is not incorporated. Weighted resample neural networks outperformed all other algorithms despite sampling and resampling variability, lending them great promise for further investigation and against more potenet competition.

## Future Work {#work}

There is massive potential in exploring the applications of neural networks to complex survey imputation. A more competitive model such as local polynomial smoothing using a validation set could be a superior general model for unknown generative functions. The linear regression proved limiting and underfitting in both simulation and CE applications. Any stable heuristic method with some potential to encode survey weights is worth investigating.

Multiple imputation is a world worth exploring due to the superior predictive capability of neural networks and their intervariable relationship preservation property. Weighted linear regression is an excellent mean estimator due to the stability of its predictions to extrema, but lacks in predictive accuracy. Neural networks, however, could offer superior multiple imputations as relationships are nonlinearized through successive predictions.

Though it performed very well on real data with the algorithm outlined in Chapter 3, the weighted resample neural network method could be improved in various ways. The method struggled in low-$n$ simulations due to highly variable resampling. This would be somewhat alleviated by resampling to $N$ weighted by $\frac{1}{\pi}$, but was cut for computational concerns. In addition to $N$-observation resampling, bootstrapping methods inspired by random forests are a promising exploration. There would be many tweaks herewithin, but the simple algorithm is to create $k$ data frames using the weighted resample `sample_n(sample, weight = 1/pi)`, then train a naive neural network to each of these datasets, and take the mean or median of the resulting imputed labels from testing data as the output prediction. This would alleviate both network and resample variability of the already-dominant algorithm.

