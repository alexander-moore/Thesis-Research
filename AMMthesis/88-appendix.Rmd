`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!--
If you feel it necessary to include an appendix, it goes here.
-->


# The First Appendix

This appendix contains all R code necessary to re-create the results of this thesis, along with supplementary data from the github. 

## Data Processing {#sec:data_process}

```{r ref.label='include_packages', results='hide', echo = TRUE, eval = FALSE}
# Make CE-data ready to load into model code

# load data
# Read in one quarter (kelly said so: can verify)

fmli171x <- read.csv("data/intrvw17/intrvw17/fmli171x.csv")

count_nas <- function(df) {
  isna <- apply(df, 2, is.na)
  tots <- apply(isna, 2, sum)
  tots
}


median_impute <- function(df) {
  
  for (i in 1:dim(df)[[2]]) {
    
    df[[i]] <- as.numeric(df[[i]])
    df[[i]][is.na(df[[i]])] <- median(df[[i]][!is.na(df[[i]])])
  }
  
  df
  
}
# Turn data set into only numerics
#dplyr::select_if(x, is.numeric)

# Impute missing columns w uninformative median imputation: preserve maximal obs
# Could revist rows with lots of missingness. ####
# Could drop missing obs that are missing our Testing Label: 
# we need actual information there for out tests
df <- fmli171x

df <- cbind(df, pi = 1 / df$FINLWT21)
which( colnames(df)=="FINLWT21")
which( colnames(df)=="pi" ) 

# Potential Labels
which( colnames(df)=="FINCBTAX" ) # (no missingness, but is BLS derived)
#AGE_REF_


count_nas(df)

## Remove columns with more than 33% NA
df <- df[, -which(colMeans(is.na(df)) > 0.33)]
count_nas(df)


# Median impute missing values, but NOT our chosen label
idf <- median_impute(df)
count_nas(idf)

write.csv(idf, file = "Data/imputed_CE.csv")
```

## Simulated Data Experiment {sim_data_exp}

```{r simulated_data_experiment, eval = FALSE, results='hide', echo = TRUE}
# model selection
# Compare distributions of mean via different imputation methods on 
#artifical data

library(keras)
library(dplyr)
library(ggplot2)
library(tensorflow)

N = 10^5
n = 10^3
it <- 100

p_y_ep_control <- 1

rescale <- function(vec) { # rescale to 0,1
  (vec - min(vec)) / (max(vec) - min(vec))
}

create_validation_split <- function(x_train, y_train) {
  # Validation Set
  # THIS HAS BEEN UPDATED TO SAMPLE 20% OF X_TRAIN ROWS AS INDECES 
  val_indices <- sample(1:nrow(x_train), .15*nrow(df))
  
  x_val <<- x_train[val_indices,]
  partial_x_train <<- x_train[-val_indices,]
  
  y_val <<- y_train[val_indices]
  partial_y_train <<- y_train[-val_indices]
}

normalize_data <- function(train_data, test_data) {
  mean <- apply(train_data, 2, mean)
  std <- apply(train_data, 2, sd)
  
  x_train <<- scale(train_data, center = mean, scale = std)
  x_test <<- scale(test_data, center = mean, scale = std)
}

p_1 <- runif(N, -30, 30)
p_2 <- runif(N, -30, 30)

nulls <- matrix(rnorm(20*N, 0, 8), ncol = 20)

p_y_ep <- rnorm(N, mean = 0, sd = p_y_ep_control)

p_y <- p_1 + abs(p_2) + p_y_ep 

p_y[p_y > mean(p_y)] <- mean(p_y)

p_y <- p_y + p_y_ep
p_y <- abs(p_y)

p_pi_ep <- rnorm(N, mean = 0, sd = 1)
temp_pi <- sqrt(p_y) + p_pi_ep
temp_pi <- rescale(temp_pi)
p_pi <- temp_pi * (n / sum(temp_pi))

p_df <- cbind(p_1, p_2, p_y, p_pi, nulls)
p_tbl <- as_tibble(p_df)

# Verify sum of pi over population approx n
sum(p_tbl$p_pi) == n

statistic_tracker <- data.frame(true_mean = numeric(it), 
                                oracle_mean = numeric(it),
                                pi_naive_mean = numeric(it),
                                median_imp_mean = numeric(it),
                                lin_imp_mean = numeric(it),
                                lin_oracle = numeric(it),
                                nn_oracle = numeric(it),
                                nn_pi_oracle = numeric(it),
                                nn_resamp_oracle = numeric(it),
                                nn_wmse_oracle = numeric(it),
                                nn_deriv_oracle = numeric(it))
# Want to estimate:
mu_y <- (1 / N) * sum(p_tbl$p_y)
mu_y
# monte carlo simulation of draws from population with comparison methods
for (i in 1:it) {
  statistic_tracker$true_mean[i] <- mu_y
  
  sample_population_by_pi <- sample_n(tbl = p_tbl, size = n, replace = FALSE, 
                                      weight = p_pi)
  
  df <- sample_population_by_pi %>% rename(x_1 = p_1, #Since we are not dealing 
                                           #with population p_ anymore
                                           x_2 = p_2,
                                           pi = p_pi,
                                           y = p_y)
  
  #########
  # Oracle mean
  # this is a baseline estimator which is done on the full (nonmissing) data
  hat_N_sample <- sum(1 / df$pi)
  statistic_tracker$oracle_mean[i] <- (1 / hat_N_sample) * sum((1 / df$pi) 
                                                               * df$y)
  
  #########
  # Drop some labels - weighted to high y
  
  indices <- sample(1:nrow(df), .2*nrow(df), prob = df$y)
  dropped_obs <- df[indices,]
  reduced_df <- df[-indices,] 
  
  # Make oracle df without noisy parameters
  odf <- df[,1:4]
  o_dropped_obs <- odf[indices,]
  o_reduced_df <- odf[-indices,]
  
  #########
  # pi-corrected naive mean
  hat_N_respondent <- sum(1 / (reduced_df$pi)) 
  statistic_tracker$pi_naive_mean[i] <- (1 / hat_N_respondent)*
    sum((1 / reduced_df$pi) * reduced_df$y)
  
  #########
  # Median imputation: fill missing values with median
  len <- dim(dropped_obs)[1]
  median_list <- rep(median(reduced_df$y), len)
  labels <- as.vector(reduced_df$y)
  median_list <- as.vector(median_list)
  
  imputed_list <- c(labels,median_list)
  
  statistic_tracker$median_imp_mean[i] <- 
    (1 / hat_N_sample) * sum((1 / df$pi) * imputed_list)
  
  #########
  # mean according to imputed data via linear regression (1/pi-weighted)
  red_weight <- 1 / reduced_df$pi
  lin_dat <- select(reduced_df, -c(pi))
  lin_dropped <- select(dropped_obs, -c(pi))
  
  linear_model <- lm(y ~ ., data = lin_dat, weights = red_weight)
  
  lm_y_hat <- predict(linear_model, lin_dropped)
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$lin_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$y / reduced_df$pi) 
                        + sum(lm_y_hat / dropped_obs$pi))
  
  # Linear Regression Oracle Method
  red_weight <- 1 / o_reduced_df$pi
  lin_dat <- select(o_reduced_df, -c(pi))
  lin_dropped <- select(o_dropped_obs, -c(pi))
  
  linear_model <- lm(y ~ ., data = lin_dat, weights = red_weight)
  
  lm_y_hat <- predict(linear_model, lin_dropped)
  hat_N_sample <- sum(1/odf$pi)
  statistic_tracker$lin_oracle[i] <- 
    (1 / hat_N_sample)*(sum(o_reduced_df$y / o_reduced_df$pi) + 
                          sum(lm_y_hat / o_dropped_obs$pi))
  
  
  # nn oracle
  y_train <- o_reduced_df$y
  reduced_df_nolab <- select(o_reduced_df, -c(pi, y))
  
  y_test <- o_dropped_obs$y
  dropped_obs_nolab <- select(o_dropped_obs, -c(pi,y))
  
  reduced_df_nolab <- as.matrix(reduced_df_nolab)
  dropped_obs_nolab <- as.matrix(dropped_obs_nolab)
  
  x_train <- reduced_df_nolab
  x_test <- dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 32, activation = "relu", 
                input_shape = dim(x_train)[[2]]) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mae")
  )
  
  history <- model %>% fit(
    partial_x_train,
    partial_y_train,
    epochs = 170, 
    verbose = 0,
    validation_data = list(x_val, y_val)
  )
  
  x_test <- as.matrix(x_test)
  nn_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_oracle[i] <- (1 / hat_N_sample)*
    (sum(o_reduced_df$y / o_reduced_df$pi) + sum(nn_y_hat / o_dropped_obs$pi))
  
  
  #oracle nn pi feature
  y_train <- o_reduced_df$y
  reduced_df_nolab <- select(o_reduced_df, -c(y))
  reduced_df_nolab$pi <- 1 / reduced_df_nolab$pi
  
  y_test <- o_dropped_obs$y
  dropped_obs_nolab <- select(o_dropped_obs, -c(y))
  dropped_obs_nolab$pi <- 1 / dropped_obs_nolab$pi
  
  reduced_df_nolab <- as.matrix(reduced_df_nolab)
  dropped_obs_nolab <- as.matrix(dropped_obs_nolab)
  
  x_train <- reduced_df_nolab
  x_test <- dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 32, activation = "relu", 
                input_shape = dim(x_train)[[2]]) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mae")
  )
  
  history <- model %>% fit(
    partial_x_train,
    partial_y_train,
    epochs = 300,
    verbose = 0,
    batch_size = 32,
    validation_data = list(x_val, y_val)
  )
  
  x_test <- as.matrix(x_test)
  nn_pi_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_pi_oracle[i] <- 
    (1 / hat_N_sample)*(sum(o_reduced_df$y / o_reduced_df$pi) + 
                          sum(nn_pi_y_hat / o_dropped_obs$pi))
  
  # oracle resample
  dropped_obs_NA_lab <- o_dropped_obs
  dropped_obs_NA_lab$y <- NA
  
  orig_df <- rbind(o_reduced_df, dropped_obs_NA_lab)
  
  weight_vec <- 1 / as.numeric(orig_df$pi)  
  
  orig_tbl <- as_tibble(orig_df)
  
  # resamples on df not reduced_df
  resamp_df <- sample_n(tbl = orig_tbl, size = nrow(orig_tbl),
                        replace = TRUE, weight = weight_vec)
  
  # re-partition into complete cases, and cases to be imputed
  resamp_reduced_df <- resamp_df[-which(is.na(resamp_df$y)),]
  resamp_dropped_obs <- resamp_df[which(is.na(resamp_df$y)),]
  
  y_train <- resamp_reduced_df$y
  resamp_reduced_df_nolab <- select(resamp_reduced_df, -c(y))
  
  y_test <- resamp_dropped_obs$y
  resamp_dropped_obs_nolab <- select(resamp_dropped_obs, -c(y))
  
  resamp_reduced_df_nolab <- as.matrix(resamp_reduced_df_nolab)
  resamp_dropped_obs_nolab <- as.matrix(resamp_dropped_obs_nolab)
  
  x_train <- resamp_reduced_df_nolab
  x_test <- resamp_dropped_obs_nolab
  
  create_validation_split(x_train, y_train)
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 32, activation = "relu", 
                input_shape = dim(x_train)[[2]]) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mae")
  )
  
  history <- model %>% fit(
    partial_x_train,
    partial_y_train,
    epochs = 200,
    verbose = 0,
    batch_size = 32,
    validation_data = list(x_val, y_val)
  )
  
  x_test <- as.matrix(x_test)
  nn_resamp_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_resamp_oracle[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$y / reduced_df$pi) + 
                          sum(nn_resamp_y_hat / resamp_dropped_obs$pi))
  
  # WSME NN 
  #######
  # New Split
  # Inherit train and test
  train <- reduced_df
  test <- dropped_obs
  
  # record training data pi
  pi_vec <- train$pi
  
  # Inherit train and test
  train <- reduced_df
  y_train <- train$FINCBTAX
  x_train <- select(train, -c(FINCBTAX))
  
  test <- dropped_obs
  y_test <- test$FINCBTAX
  x_test <- select(test, -c(FINCBTAX))
  
  # Split train into train' and val
  val_indices <- sample(1:nrow(train), .20*nrow(df))
  
  x_val <- x_train[val_indices,]
  partial_x_train <- x_train[-val_indices,]
  
  y_val <- y_train[val_indices]
  partial_y_train <- y_train[-val_indices]
  
  
  # Extract partial pi from full train x
  pi_prime <- pi_vec[-val_indices]
  obs_weights <- 1 / pi_prime
  
  full_obs_weights <- 1 / pi_vec
  
  # remove pi from x_test and x_train, partial_x_train, x_val
  x_test <- select(x_test, -c(pi))
  x_train <- select(x_train, -c(pi))
  x_val <- select(x_val, -c(pi))
  partial_x_train <- select(partial_x_train, -c(pi))
  
  # Normalize all WRT train
  mean <- apply(x_train, 2, mean)
  std <- apply(x_train, 2, sd)
  
  x_train <- scale(x_train, center = mean, scale = std)
  x_test <- scale(x_test, center = mean, scale = std)
  x_val <- scale(x_val, center = mean, scale = std)
  partial_x_train <- scale(partial_x_train, center = mean, scale = std)
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 32, activation = "relu", 
                input_shape = dim(x_train)[[2]]) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mae")
  )
  
  history <- model %>% fit(
    partial_x_train,
    partial_y_train,
    sample_weight = obs_weights,
    epochs = 250,
    verbose = 0,
    batch_size = 32,
    validation_data = list(x_val, y_val)
  )
  
  x_test <- as.matrix(x_test)
  x_test <- x_test[,-3]
  nn_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_wmse_oracle[i] <- 
    (1 / hat_N_sample)*(sum(o_reduced_df$y / o_reduced_df$pi) 
                        + sum(nn_y_hat / o_dropped_obs$pi))
  
  
  # Oracle derived params
  y_train <- o_reduced_df$y
  reduced_df_nolab <- select(o_reduced_df, -c(y))
  
  y_test <- o_dropped_obs$y
  dropped_obs_nolab <- select(o_dropped_obs, -c(y))
  
  x1pi <- (reduced_df_nolab$x_1)*(reduced_df_nolab$pi)
  x2pi <- (reduced_df_nolab$x_2)*(reduced_df_nolab$pi)
  
  dx1pi <- (dropped_obs_nolab$x_1)*(dropped_obs_nolab$pi)
  dx2pi <- (dropped_obs_nolab$x_2)*(dropped_obs_nolab$pi)
  
  reduced_df_nolab <- cbind(reduced_df_nolab, x1pi, x2pi)
  dropped_obs_nolab <- cbind(dropped_obs_nolab, dx1pi, dx2pi)
  
  reduced_df_nolab <- as.matrix(reduced_df_nolab)
  dropped_obs_nolab <- as.matrix(dropped_obs_nolab)
  
  x_train <- reduced_df_nolab
  x_test <- dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 32, activation = "relu", 
                input_shape = dim(x_train)[[2]]) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mae"))
  
  history <- model %>% fit(
    partial_x_train,
    partial_y_train,
    epochs = 300, 
    verbose = 0,
    batch_size = 32, 
    validation_data = list(x_val, y_val)  
  )
  
  x_test <- as.matrix(x_test)
  nn_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_deriv_oracle[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$y / reduced_df$pi)
                        + sum(nn_y_hat / dropped_obs$pi))
  
  
  print(i)
}

# Save the mean table so that we don't have to always re-run it
write.csv(statistic_tracker, file = "C:\\Users\\Alexander\\Documents\\thesis stat tracker\\AWS_ALEXHERE_2.csv")
dat <- statistic_tracker

#dat <- read.csv("c:/Users/Alexander/Documents/thesis stat tracker/10_full.csv")
#dat <- dat[,-1]

#### COMPUTING MSE (and making table)
# MSE against TRUE mean for each method:
# take dif of each with true. square each. sum all. divide by iterations

mse_table <- dat[1,]
for (i in 1:dim(dat)[2]) {
  
  matdat <- as.matrix(dat)
  
  mse_table[i] <- mean( (matdat[,i] - matdat[,1])^2 )
}

mse_table

# then do MSE of method divided by MSE of oracle to get measure of goodness
# (closer to 1 means closer to oracle means good)

oracle_ratio_table <- dat[1,]

for (i in 1:dim(dat)[2]) {
  
  oracle_ratio_table[i] <- mse_table[i] / mse_table[2]
}

oracle_ratio_table

# compute bias:
# mean of statistic vector - true mean
# divide this differnce by true mean
# multiple by 100 to get "percent relative bais", a normalized bias measure

prb_table <- dat[1,]
mu_y <- mean(dat[,1])

for (i in 1:dim(dat)[2]) {
  
  prb_table[i] <- 100 * ((mean(dat[,i]) - mu_y) / mu_y)
}

prb_table

library(kableExtra)
library(knitr)

mse_t <- transpose(mse_table)
oracle_t <- transpose(oracle_ratio_table)
prb_t <- transpose(prb_table)

binded <- cbind(mse_t, oracle_t, prb_t)
colnames(binded) <- c("MSE", "Oracle Ratio", "Relative Bias")
rownames(binded) <- colnames(statistic_tracker)

test <- binded
test <- signif(test, digits = 3)

test %>%
  kable() %>%
  kable_styling()

```

## CE Data Experiment {ce_data_exp}
```{r ce_data_experiment, eval = FALSE, echo = TRUE}
## Real Experiments in CE Data
# Documentation:
# https://www.bls.gov/cex/2017/csxintvwdata.pdf

library(readr)
library(dplyr)
library(keras)
library(kableExtra)
library(knitr)
library(ggplot2)
library(tensorflow)
library(data.table)

# Relevant functions
count_nas <- function(df) {
  isna <- apply(df, 2, is.na)
  tots <- apply(isna, 2, sum)
  tots
}

rescale <- function(vec) { # rescale to 0,1
  (vec - min(vec)) / (max(vec) - min(vec))
}

create_validation_split <- function(x_train, y_train) {
  # Validation Set
  val_indices <- sample(1:nrow(x_train), .2*nrow(df))
  
  x_val <<- x_train[val_indices,]
  partial_x_train <<- x_train[-val_indices,]
  
  y_val <<- y_train[val_indices]
  partial_y_train <<- y_train[-val_indices]
}

normalize_data <- function(train_data, test_data) {
  mean <- apply(train_data, 2, mean)
  std <- apply(train_data, 2, sd)
  
  x_train <<- scale(train_data, center = mean, scale = std)
  x_test <<- scale(test_data, center = mean, scale = std)
}

# custom `adam` optimizer, learning rate up for grabs
# .001 default, .005 testing (methods were using 600 epo)
adam_lr <- optimizer_adam(lr = 0.1, beta_1 = 0.9, beta_2 = 0.999,
                          epsilon = NULL, decay = 0, amsgrad = FALSE, 
                          clipnorm = NULL, clipvalue = NULL)

dat <- read_csv("Data/imputed_CE.csv")

# Assuming data is in the correct shape, here's how the real testing goes down
it <- 20

statistic_tracker <- data.frame(true_mean = numeric(it), 
                                oracle_mean = numeric(it),
                                pi_naive_mean = numeric(it),
                                median_imp_mean = numeric(it),
                                lin_imp_mean = numeric(it),
                                nn_imp_mean = numeric(it),
                                nn_pi_imp_mean = numeric(it),
                                nn_resamp_imp_mean = numeric(it),
                                nn_wmse_imp_mean = numeric(it),
                                nn_deriv_imp_mean = numeric(it))

# Prep label
df <- select(dat, c(FINCBTAX, pi, AGE_REF, BATHRMQ, BEDROOMQ, EDUC_REF, 
                    FAM_SIZE, TOTEXPCQ))

#df <- df[1:1000,]

#label <- "FINCBTAX"
#label_index <- which(colnames(df) == label)

mu_y <- mean(df$FINCBTAX)
mu_y # == mean american household income :)

testing <- TRUE
#epo <- 200

for (i in 1:it) {
  
  # Split the CE data into training and testing. Might want some kind of scheme 
  # to get the right amount of high-pi and low-pi observations in each
  # (testing is unlabelled, but we know the right answer)
  
  #########
  # True mean
  statistic_tracker$true_mean[i] <- mu_y
  
  #########
  # Oracle mean
  # this is a baseline estimator which is done on the full (nonmissing) data
  hat_N_sample <- sum(1 / df$pi)
  statistic_tracker$oracle_mean[i] <- 
    (1 / hat_N_sample) * sum((1 / df$pi) * df$FINCBTAX)
  
  #########
  # Drop some labels - weighted to high y
  zeros <- rep(0, nrow(df))
  rec <- pmax(zeros, df$FINCBTAX)
  
  indices <- sample(1:nrow(df), .20*nrow(df), prob = rec)
  
  # Make df with all features (noisy)
  dropped_obs <- df[indices,]
  reduced_df <- df[-indices,] 
  
  #########
  # pi-weighted naive mean
  # non-imputation of estimate which accounts for complex design but ignores systematic
  hat_N_respondent <- sum(1 / (reduced_df$pi))
  statistic_tracker$pi_naive_mean[i] <- 
    (1 / hat_N_respondent)*sum((1 / reduced_df$pi) *  reduced_df$FINCBTAX)
  
  # Compute median imputation mean estimate
  
  #########
  # Median imputation: fill missing values with median
  len <- dim(dropped_obs)[1]
  median_list <- rep(median(reduced_df$FINCBTAX), len)
  labels <- as.vector(reduced_df$FINCBTAX)
  median_list <- as.vector(median_list)
  
  imputed_list <- c(labels,median_list)
  
  statistic_tracker$median_imp_mean[i] <- 
    (1 / hat_N_sample) * sum((1 / df$pi) *  imputed_list)
  
  #########
  # Linear regression imputation: MSE 1/pi weighted
  red_weight <- 1 / reduced_df$pi
  lin_dat <- select(reduced_df, -c(pi))
  lin_dropped <- select(dropped_obs, -c(pi))
  
  linear_model <- lm(lin_dat$FINCBTAX ~ ., data = lin_dat, weights = red_weight)
  
  lm_y_hat <- predict(linear_model, lin_dropped) # THROWS WARN
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$lin_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$FINCBTAX / reduced_df$pi) + 
                          sum(lm_y_hat / dropped_obs$pi))
  
  #########
  # Mean accoridng to imputed data via naive neural network (ignore complex design)
  y_train <- reduced_df$FINCBTAX
  reduced_df_nolab <- select(reduced_df, -c(pi, FINCBTAX))
  
  y_test <- dropped_obs$FINCBTAX
  dropped_obs_nolab <- select(dropped_obs, -c(pi, FINCBTAX))
  
  reduced_df_nolab <- as.matrix(reduced_df_nolab)
  dropped_obs_nolab <- as.matrix(dropped_obs_nolab)
  
  x_train <- reduced_df_nolab
  x_test <- dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  if(testing){
    # Intentionally over-train on the training data to find the validation min
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    epo <- 150
    history <- model %>% fit(
      partial_x_train,
      partial_y_train,
      epochs = epo, 
      verbose = 0,
      validation_data = list(x_val, y_val)
    )
    
    goodtrain <- which.min(history$metrics$val_loss)
    print(goodtrain)
    print("of")
    print(epo)
    
    # Train a new model for the validation-minimizing number of epochs
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = goodtrain, 
      verbose = 0
      #batch_size = 32,
      #validation_data = list(x_val, y_val)
    )
    plot(history)
  } else {
  
    # Train a new model for the validation-minimizing number of epochs
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 419, 
      verbose = 0
      #callbacks = EarlyStopping()
      #batch_size = 32,
      #validation_data = list(x_val, y_val)
    )
  }
  
  x_test <- as.matrix(x_test)
  nn_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$FINCBTAX / reduced_df$pi) + 
                          sum(nn_y_hat / dropped_obs$pi))
  
  print("Finished nn imp mean")
  
  #########
  # Mean according to imputed data via neural network w pi feature
  y_train <- reduced_df$FINCBTAX
  reduced_df_nolab <- select(reduced_df, -c(FINCBTAX))
  
  y_test <- dropped_obs$FINCBTAX
  dropped_obs_nolab <- select(dropped_obs, -c(FINCBTAX))
  
  reduced_df_nolab <- as.matrix(reduced_df_nolab)
  dropped_obs_nolab <- as.matrix(dropped_obs_nolab)
  
  x_train <- reduced_df_nolab
  x_test <- dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  if(testing){
    # intentionally over-train the model
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    epo <- 100
    history <- model %>% fit(
      partial_x_train,
      partial_y_train,
      epochs = epo,
      verbose = 0,
      validation_data = list(x_val, y_val)
    )
    
    goodtrain <- which.min(history$metrics$val_loss)
    print(goodtrain)
    print("of")
    print(epo)
    
    # re-train the new model to the val-min
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = goodtrain, 
      verbose = 0
    )
    plot(history)
    
  } else {
  
    # re-train the new model to the val-min
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 218, 
      verbose = 0
      #batch_size = 32,
      #validation_data = list(x_val, y_val)
    )
  }
  
  x_test <- as.matrix(x_test)
  nn_pi_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_pi_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$FINCBTAX / reduced_df$pi) + 
                          sum(nn_pi_y_hat / dropped_obs$pi))
  
  print("Finished nn pi imp mean")
  
  
  # WSME NN 
  #######
  # New Split
  # Inherit train and test
  train <- reduced_df
  test <- dropped_obs
  
  # record training data pi
  pi_vec <- train$pi
  
  # Inherit train and test
  train <- reduced_df
  y_train <- train$FINCBTAX
  x_train <- select(train, -c(FINCBTAX))
  
  test <- dropped_obs
  y_test <- test$FINCBTAX
  x_test <- select(test, -c(FINCBTAX))
  
  # Split train into train' and val
  val_indices <- sample(1:nrow(train), .20*nrow(df))
  
  x_val <- x_train[val_indices,]
  partial_x_train <- x_train[-val_indices,]
  
  y_val <- y_train[val_indices]
  partial_y_train <- y_train[-val_indices]
  
  
  # Extract partial pi from full train x
  pi_prime <- pi_vec[-val_indices]
  obs_weights <- 1 / pi_prime

  full_obs_weights <- 1 / pi_vec

  # remove pi from x_test and x_train, partial_x_train, x_val
  x_test <- select(x_test, -c(pi))
  x_train <- select(x_train, -c(pi))
  x_val <- select(x_val, -c(pi))
  partial_x_train <- select(partial_x_train, -c(pi))
  
  # Normalize all WRT train
  mean <- apply(x_train, 2, mean)
  std <- apply(x_train, 2, sd)
  
  x_train <- scale(x_train, center = mean, scale = std)
  x_test <- scale(x_test, center = mean, scale = std)
  x_val <- scale(x_val, center = mean, scale = std)
  partial_x_train <- scale(partial_x_train, center = mean, scale = std)
  
  #######
  
  if(testing){
    # intentionally overtrain
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(partial_x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    epo <- 100
    history <- model %>% fit(
      partial_x_train,
      partial_y_train,
      sample_weight = obs_weights,
      epochs = epo,
      verbose = 0,
      validation_data = list(x_val, y_val)
    )
    
    goodtrain <- which.min(history$metrics$val_loss)
    print(goodtrain)
    print("of")
    print(epo)
    
    # retrain 
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = goodtrain, 
      verbose = 0,
      sample_weight = full_obs_weights
    )
    plot(history)
    
  } else {
  
    # retrain 
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 209, 
      verbose = 0,
      sample_weight = full_obs_weights
    )
  }
  
  nn_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_wmse_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$FINCBTAX / reduced_df$pi) + 
                          sum(nn_y_hat / dropped_obs$pi))
  
  print("Finished nn wmse mean")
  
  
  ##########
  # Mean according to imputed dataset via neural network with weighted resample
  # on the full sample. the missing values in the resample are then imputed and 
  # the imputation mean is taken on the new data set
  # (without pi feature)
  dropped_obs_NA_lab <- dropped_obs
  dropped_obs_NA_lab$FINCBTAX <- NA
  
  orig_df <- rbind(reduced_df, dropped_obs_NA_lab)
  
  # re-sample by inclusion probability 
  weight_vec <- 1 / as.numeric(orig_df$pi)  
  
  orig_tbl <- as_tibble(orig_df)
  
  # RESAMPLE ON DF NOT REDUCED_DF
  resamp_df <- sample_n(tbl = orig_tbl, size = nrow(orig_tbl),
                        replace = TRUE, weight = weight_vec)
  
  # re-partition into complete cases, and cases to be imputed
  resamp_reduced_df <- resamp_df[-which(is.na(resamp_df$FINCBTAX)),]
  resamp_dropped_obs <- resamp_df[which(is.na(resamp_df$FINCBTAX)),]
  
  y_train <- resamp_reduced_df$FINCBTAX
  resamp_reduced_df_nolab <- select(resamp_reduced_df, -c(FINCBTAX))
  
  y_test <- resamp_dropped_obs$FINCBTAX
  resamp_dropped_obs_nolab <- select(resamp_dropped_obs, -c(FINCBTAX))
  
  resamp_reduced_df_nolab <- as.matrix(resamp_reduced_df_nolab)
  resamp_dropped_obs_nolab <- as.matrix(resamp_dropped_obs_nolab)
  
  x_train <- resamp_reduced_df_nolab
  x_test <- resamp_dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  if(testing){
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    epo <- 200
    history <- model %>% fit(
      partial_x_train,
      partial_y_train,
      epochs = epo,
      verbose = 0,
      validation_data = list(x_val, y_val)
    )
    
    goodtrain <- which.min(history$metrics$val_loss)
    print(goodtrain)
    print("of")
    print(epo)
    
    # retrain 
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = goodtrain, 
      verbose = 0
    )
    plot(history)
    
  } else {
  
    # retrain 
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 800, 
      verbose = 0
      #batch_size = 32,
      #validation_data = list(x_val, y_val)
    )
  }
  
  x_test <- as.matrix(x_test)
  nn_resamp_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_resamp_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$FINCBTAX / reduced_df$pi) + 
                          sum(nn_resamp_y_hat / resamp_dropped_obs$pi))
  
  print("Finished resamp imp mean")
  
  
  ########
  # Derived-parameter NN imputation, where derived parameters are ??
  
  y_train <- reduced_df$FINCBTAX
  reduced_df_nolab <- select(reduced_df, -c(FINCBTAX))
  
  y_test <- dropped_obs$FINCBTAX
  dropped_obs_nolab <- select(dropped_obs, -c(FINCBTAX))
  
  x1pi <- (reduced_df_nolab$TOTEXPCQ)*(reduced_df_nolab$pi)
  x2pi <- (reduced_df_nolab$AGE_REF)*(reduced_df_nolab$pi)
  
  dx1pi <- (dropped_obs_nolab$TOTEXPCQ)*(dropped_obs_nolab$pi)
  dx2pi <- (dropped_obs_nolab$AGE_REF)*(dropped_obs_nolab$pi)
  
  reduced_df_nolab <- cbind(reduced_df_nolab, x1pi, x2pi)
  dropped_obs_nolab <- cbind(dropped_obs_nolab, dx1pi, dx2pi)
  
  reduced_df_nolab <- as.matrix(reduced_df_nolab)
  dropped_obs_nolab <- as.matrix(dropped_obs_nolab)
  
  x_train <- reduced_df_nolab
  x_test <- dropped_obs_nolab
  
  normalize_data(x_train, x_test)
  create_validation_split(x_train, y_train)
  
  if(testing){
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae"))
    
    epo <- 100
    history <- model %>% fit(
      partial_x_train,
      partial_y_train,
      epochs = epo,
      verbose = 0,
      #batch_size = 32, 
      validation_data = list(x_val, y_val)  
    )
    
    goodtrain <- which.min(history$metrics$val_loss)
    print(goodtrain)
    print("of")
    print(epo)
    
    # retrain 
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = goodtrain, 
      verbose = 0
      #batch_size = 32,
      #validation_data = list(x_val, y_val) 
    )
    plot(history)
    
  } else {
    # retrain 
    model <- keras_model_sequential() %>%
      layer_dense(units = 64, activation = "relu", 
                  input_shape = dim(x_train)[[2]]) %>%
      layer_dense(units = 64, activation = "relu") %>%
      layer_dense(units = 1)
    
    model %>% compile(
      optimizer = adam_lr,
      loss = "mse",
      metrics = c("mae")
    )
    
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 165, 
      verbose = 0
      #batch_size = 32,
      #validation_data = list(x_val, y_val) 
    )
  }
  
  x_test <- as.matrix(x_test)
  nn_y_hat <- predict(model, x_test)
  
  hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_deriv_imp_mean[i] <- 
    (1 / hat_N_sample)*(sum(reduced_df$FINCBTAX / reduced_df$pi) + 
                          sum(nn_y_hat / dropped_obs$pi))
  print("Finished derived param imp mean")
  
  print(i)
}

statistic_tracker %>% write.csv(file = 
            "C:\\Users\\Alexander\\Documents\\thesis stat tracker\\LRepo_8.csv")



################################################
# Compare results
# oracle only






dat <- statistic_tracker

mse_table <- dat[1,]
for (i in 1:dim(dat)[2]) {
  
  matdat <- as.matrix(dat)
  
  mse_table[i] <- mean( (matdat[,i] - matdat[,1])^2 )
}

mse_table

# then do MSE of method divided by MSE of oracle to get measure of goodness
# (closer to 1 means closer to oracle means good)

oracle_ratio_table <- dat[1,]

for (i in 1:dim(dat)[2]) {
  
  oracle_ratio_table[i] <- mse_table[i] / mse_table[2]
}

oracle_ratio_table


# compute bias:
# mean of statistic vector - true mean
# divide this differnce by true mean
# multiple by 100 to get "percent relative bais", a normalized bias measure

prb_table <- dat[1,]
mu_y <- mean(dat[,1])

for (i in 1:dim(dat)[2]) {
  
  prb_table[i] <- 100 * ((mean(dat[,i]) - mu_y) / mu_y)
}

prb_table

mse_t <- transpose(mse_table)
oracle_t <- transpose(oracle_ratio_table)
prb_t <- transpose(prb_table)

binded <- cbind(mse_t, oracle_t, prb_t)
colnames(binded) <- c("MSE", "Oracle Ratio", "Relative Bias")
rownames(binded) <- colnames(statistic_tracker)

test <- binded
test <- signif(test, digits = 3)

test %>%
  kable() %>%
  kable_styling()

if(FALSE){
  # mean and SD ratios
  df <- read.csv("Data/AWS_running_short.csv")
  df <- df[,-1]
  
  means <- apply(df, FUN = mean, MARGIN = 2)
  means
  
  sds <- apply(df, FUN = sd, MARGIN = 2)
  sds
  
  mean_ratio <- means / means[2]
  mean_ratio
  
  sd_ratio <- sds / sds[2]
  sd_ratio
  
  # fULL
  df <- read.csv("Data/AWS_running_full.csv")
  df <- df[,-1]
  
  means <- apply(df, FUN = mean, MARGIN = 2)
  means
  
  sds <- apply(df, FUN = sd, MARGIN = 2)
  sds
  
  mean_ratio <- means / means[2]
  mean_ratio
  
  sd_ratio <- sds / sds[2]
  sd_ratio

}


```
