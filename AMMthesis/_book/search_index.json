[
["index.html", "Neural Network Methods in Complex Survey Imputation Introduction", " Neural Network Methods in Complex Survey Imputation Alexander Michael Moore May 2019 Introduction Welcome to the R Markdown thesis template. This template is based on (and in many places copied directly from) the Reed College LaTeX template, but hopefully it will provide a nicer interface for those that have never used TeX or LaTeX before. Using R Markdown will also allow you to easily keep track of your analyses in R chunks of code, with the resulting plots and output included as well. The hope is this R Markdown template gets you in the habit of doing reproducible research, which benefits you long-term as a researcher, but also will greatly help anyone that is trying to reproduce or build onto your results down the road. Hopefully, you won’t have much of a learning period to go through and you will reap the benefits of a nicely formatted thesis. The use of LaTeX in combination with Markdown is more consistent than the output of a word processor, much less prone to corruption or crashing, and the resulting file is smaller than a Word file. While you may have never had problems using Word in the past, your thesis is likely going to be about twice as large and complex as anything you’ve written before, taxing Word’s capabilities. After working with Markdown and R together for a few weeks, we are confident this will be your reporting style of choice going forward. Why use it? R Markdown creates a simple and straightforward way to interface with the beauty of LaTeX. Packages have been written in R to work directly with LaTeX to produce nicely formatting tables and paragraphs. In addition to creating a user friendly interface to LaTeX, R Markdown also allows you to read in your data, to analyze it and to visualize it using R functions, and also to provide the documentation and commentary on the results of your project. Further, it allows for R results to be passed inline to the commentary of your results. You’ll see more on this later. Who should use it? Anyone who needs to use data analysis, math, tables, a lot of figures, complex cross-references, or who just cares about the final appearance of their document should use R Markdown. Of particular use should be anyone in the sciences, but the user-friendly nature of Markdown and its ability to keep track of and easily include figures, automatically generate a table of contents, index, references, table of figures, etc. should make it of great benefit to nearly anyone writing a thesis project. "],
["1-complex-surveys.html", "Chapter 1 Complex Surveys 1.1 Survey Statistics 1.2 Imputation", " Chapter 1 Complex Surveys 1.1 Survey Statistics Researchers in the social science and health sciences are increasingly interested in using data from complex surveys to conduct the same sorts of analyses that they traditionally conduct with more straightforward data. Medical researchers are also increasingly aware of the advantages of well-designed subsamples when measuring novel, expensive variables on an existing cohort. - Lumley 1969 The implicit pervasiveness of survey statistics in all data motivates our exploration into its significance in imputation. Survey statistics differ from statistics modelling in the specification of the random process that generates the data. In model-based statistics, some underlying generative model from which observations are drawn is assumed to exist. By understanding or approximating this model from data, one may draw conclusions on the nature of the generative function provided no meaningful changes to the data are made. Contrary to model-based statistics, the analysis of complex survey samples are design based. The observations from a researcher-specified population have fixed features, and randomness is introduced when these observations are drawn from the population according to some stochastic design. This random process is under the control of the researcher, and can be known precisely. The significance of design-based methods is that the probability sample is the procedure for taking samples from a population, not just the resulting data. Using this method, the features of the population from which the observations are drawn may be estimated, but these conclusions may not generalize to other populations. With understanding of the survey design from which data observations arise, the researcher may make improved estimates of the population of study (Lumley, 2011). The probability sample is the fundamental concept of design-based inference. Taking a random sample of 36,000 people from Oregon is an example of a survey design which implies independent and equal probability sampling of all humans in the state. The Law of Large Numbers is invoked to assume the distribution of sampled observations represent the population from which they are drawn according to any features of interest to the researcher, such as height, weight, or age. This type of surveying can be complicated by adding a stratifying element, such as randomly sampling 1,000 people from each of the 36 counties of Oregon. The data created by such a design would likely not be representative of the state, since people from lower-population counties would be more likely to be sampled. However, since the probability of each person in the sample being randomly selected is known (since the population of each county is known), this is still a probability sample. The key point of this process is that a probability sample is the procedure for taking samples from a population, not “just the data we happen to end up with” (Lumley, 2011). There are four requirements for a data set to be a probability sample: Every individual in the population must have a non-zero probability of ending up in the sample. The probability of inclusion must be known for every individual who does end up in the sample. Every pair of individuals in the sample must have a non-zero probability of both ending up in the sample. The probability of every pair of individuals being included in the sample must be known for every pair of individuals in the sample. The fundamental statistical idea behind all of design-based inference is that an individual sampled with a sampling probability \\(\\pi_i\\) represents \\(\\frac{1}{\\pi_i}\\) individuals in the population. The value \\(\\frac{1}{\\pi_i}\\) is called the sampling weight (Lumley, 2011). 1 and 2 are necessary in order to get valid population estimates, 3 and 4 are necessary to work out the accuracy of the estimates. If individuals were sampled independently of each other the first two properties would guarantee the last two (Lumley, 2011). Though 3 and 4 are requirements of a probability sample, they are often not included in datasets as they require an \\(n \\times n\\) matrix of probabilities, where \\(n\\) is the number of observations in the data set. Data collected under a complex survey design have an additional layer of complexity and are not to be treated as typical independent and identically distributed (i.i.d.) data. Ignoring this complex survey design is found to create significant error in data analyses (Toth &amp; Eltinge, 2011). This concern motivates our exploration of accounting for survey design in neural network imputation. 1.2 Imputation Often in real-world data, there is some degree of missingness. This can be for any number of reasons, illustrated in ??: Table 1.1: Types of missingess Description Noncoverage An element in the target population is not included in the survey sampling frame Total Nonresponse A sampled element does not participate in the survey Item Nonresponse A responding sampled element fails to provide acceptable responses to one or more of the survey items Table: Types of missingness “The usual form of item nonresponse is imputation, which involves assigning a value for the missing response.” 216 (Brick and Kalton 1996) In many statistical analyses, observations with any degree of missingness cannot be present. For example, how would one perform a linear regression with observations that have features but no label? Dropping these terms would remove potentially huge swathes of the data set, particularly in multivariate data sets, and would create systematic bias. Suppose for example a data set with information on roses had a feature with stem length and a label on flower size. Missing values for flower size might not be randomly distributed: there could, for example, be a systematic picking of the large-flowered roses by passersby. To ignore these observations would lead the analyst to draw false conclusions on the relationship of stem length to flower size, and the distribution of flower sizes in the population. Consider ??: Imputation attempts to address this common dilemma in real-world data. Imputation is the process of replacing missingness in data sets with some value that redeems the observation for some degree of analysis. “The aim of these methods is to compensate for the missing data in such a manner that the analysis file may be subjected to any form of analysis without the need for further consideration of the missing data” (Brick &amp; Kalton, 1996). Imputation assigns values to missing responses, which allows records with missingness to be retained during analysis. Ideally, imputation would eliminate bias in survey estimates caused by ignoring records with missing data. The catch is that imputation can destroy intervariable relationships, and by nature data is fabricated in the process which leads to overestimation of the precision of survey estimates. There are stochastic and deterministic methods for imputation. Deterministic regression imputation is the predicted value from a regression trained on complete-case data. Stochastic imputation differs due to an additional residual added term to the predicted value, taken either from a random respondent or comparable observation, and is usually preferred due to the importance of shape parameters in many analyses (Brick &amp; Kalton, 1996). One traditional method of imputation is the “Hot-Deck Method”. This method was generally favorable when computation was at an expense, and required extensive knowledge of the survey variables in order to optimize, as explicit model-based imputation needs a valid model for every survey variable (Maiti, Miller, &amp; Mukhopadhyay, 2008). This thesis proposes naive artificial neural networks as a solution which requires minimal domain knowledge and resists the curse of dimensionality which other nonparametric methods are susceptible to, such as local polynomial regression (Maiti et al., 2008). Due to the difficulties imposed by working with high-dimensional, complex survey data with varying degrees of domain knowledge, we turn to neural networks and their structural benefits to reliably perform these challenging imputation problems. "],
["2-math-sci.html", "Chapter 2 Neural Networks 2.1 Introduction to Machine Learning 2.2 Neural Networks", " Chapter 2 Neural Networks 2.1 Introduction to Machine Learning From the advent of computation, there has been a drive towards automation. (Goodfellow, Bengio, &amp; Courville, 2016) The capacity to derive patterns from raw data is known as machine learning, a broad term for an extremely diverse collection of algorithms. Machine learning flips the script on classical programming: whereas classical programming takes rules and data to produce answers, machine learning creates rules from data and answers by being “trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds a structure that derives rules for automating the task” (Chollet &amp; Allaire, 2018). Machine learning is intended to elucidate or predict patterns in data. These algorithms handle anything from linear regression for predicting a numerical response based on a number of predictors, to clustering for visualization and assignment of untaught observation classes. The models are trained by minimizing error during exposure to labelled training data with some underlying distribution and random noise, then passed unlabelled test data to predict the corresponding unknown class or value. Predictive (or supervised) machine learning algorithms seek to emulate and elucidate a true unknown generative function from which the data were drawn. For imputation purposes, our goal will be to accurately estimate missing values by approximating the generative function from which they are drawn. Generative functions have the following form \\[ y = f(x_1, x_2, .) + \\epsilon \\] Where the true label \\(y\\) of the observation is a function of the features \\(x_1, ..., x_n\\) perturbed by some random noise \\(\\epsilon\\). The estimating model will be trained via exposure to labelled observations, called the training data, then used to predict observations with missing labels, called testing data. The challenge in machine learning is learning the correct amount from training data, in order to derive the underlying distribution of the observations without simply memorizing the labels of the training set. This problem is called overfitting and is central to machine learning. 2.1 is an illustration of this pervasive principle in a regression example. An overfit (or too-flexible) model simply learns the observation’s labels, rather than the underlying distribution (or generative function, in this case a second-degree polynomial). The overfit model fails to learn the underlying generative function, and instead learns the random noise of the training observations, and thus is a poor explanation of a new realization from the same generative function, seen in 2.2: Figure 2.1: The overfit model has extremely low error on the realization of the data on which it was trained. Figure 2.2: However, the overfit model does not accurately reflect the underlying distribution from which both datasets are drawn. Rather, it captures only the random noise of the data on which it was trained. It would be a mistake to assume the generative function is a degree 15 polynomial just because the training error of such an approximation is low. An underfit model 2.3 also fails to capture the underlying distribution, due to a lack of flexibility. A linear regression, though it attempts to minimize its training MSE, clearly fails to capture the underlying distribution of the data: Figure 2.3: An underfit model fails to capture the underlying distribution 2.3 A well-trained model which straddles these extrema captures the apparent underlying distribution of the data in a general sense by approximation the generative function from which they are drawn, and remains fairly constant under different draws from the same distribution: Figure 2.4: This model has the correct level of flexibility, and accurately captures the underlying generative function while avoiding overtraining based on noise. This property gives it superior generalizability to new realizations of the data, which it was not trained on. Figure 2.4: This model has the correct level of flexibility, and accurately captures the underlying generative function while avoiding overtraining based on noise. This property gives it superior generalizability to new realizations of the data, which it was not trained on. Figure 2.5: http://scott.fortmann-roe.com/docs/BiasVariance.html 2.5 demonstrates the involvement of model complexity (or flexibility) in terms of training error. Model complexity refers to the ability of the model to approximate complex generative functions. We see that as the flexibility of the model increases, the bias on the training set always decreases, representing the model’s performance on observations with labels. However,complexity beyond the models optimal value implies over-flexibility, in which the model is able to memorize random noise rather than stopping at the trend of the data. This increases the total error if the model when exposed to data that comes from the same generative function that the model has not been exposed to, such as a testing data set or another observation from outside the training set. Higher flexibility models create more variable models, which though trained on data from the same generative function, differ greatly in appearance due to the random sample taken from the population which makes our training data, and are unstable for inference and generalizable predictivity. One key difference from statistical modelling and survey statistics methods is the point at which randomness is introduced into the data. Machine learning attempts to approximate the function from which the data was randomly generated, while survey statistics imply that randomness in data comes from the survey design. The paradox of where randomness is introduced into the data is resolved with the existence of a superpopulation \\(U\\), where each observation has label \\(y = f(x) + \\epsilon\\), some generative function. From this superpopulation, a population \\(u\\) is created through i.i.d. realizations from \\(U\\). From this population \\(u\\), the survey is taken. Thus there still exists a generative function from which the population is drawn, but the features and label of the observations are fixed by the time the complex survey is taken on the population, reconciling the two methodologies. 2.2 Neural Networks 2.2.1 Background and Context Neural networks are a family of machine learning algorithms with an extended and diverse history of research in neuroscience, statistics, and computer science. Recently, these models have experienced great growth in attention and popularity due to the contemporary circumstance of great strides in computational capability. Neural networks thrives on large training data sets, which have trended to increase in size and availability throughout time, and alongside data sets with huge amounts of observations and predictors are more sophisticated deep learning models. Neural networks also outperform competing algorithms in high-dimensional feature problems, which are common in real-world machine learning applications such as image data, waveform data, and large surveys. Often utilizing derivatives of complex compositions of functions as an optimization method, deep learning training periods are computationally intensive, relying heavily on computer hardware and optimized software for reasonable implementation. Lastly, recent attention to and development of neural networks can be attributed to their proven potential in solving complicated real-world applications with promising and increasingly dominant accuracy in practice, often at the cost of a lack of inferability. This lack of infer-ability is the typical downside of working with neural networks as the inference on a model can be more important than the predictive accuracy depending on the problem. Once a simple linear regression is trained, the coefficients on the predictors offer an immediately understandable interpretation of the behavior of the data. For example, a coefficient of .3 on a predictor \\(x\\) has a simple, instant understanding: as predictor \\(x\\) goes up by 1, the response goes up by .3. Neural networks however, lack this instant recognition due to the less intuitive layered structure of input transformations, known as representation learning. 2.2.2 Basics Neural Networks are a composition of functions. The following describes a full neural network: \\[ \\hat y = f(x; \\theta, \\omega) = f^n ( f^{n-1} ( . f^1(x))) \\] In this function, we see the input features \\(x\\), the learned coefficients \\(\\theta\\), the learning rate \\(\\omega\\), and the output prediction \\(\\hat{y}\\). Consider one of the layers of the network, \\(f_i\\). This layer is an activation function on linear function: \\[ f^i = \\max ( 0 , {W_i}^T x + c_i) \\] The activation function shown above is the rectified linear unit, or \\(\\max(0,a)\\). Activation functions are significant as they introduce nonlinearity into what would otherwise be a linear function (a composition of linear functions). \\({W_i}^T\\) and \\(c\\) in \\(f_i\\) dictate a linear transformation on the input to the layer an ordered list of all elements of \\(W_i\\) and \\(c_i\\) for all \\(i \\in n\\) would give the full description of the network, called \\(\\theta\\). So the output of a 1-layer network can be expressed as \\[ f(x; W, c, w, b) = w^T \\max( 0 , W^T x +c ) +b \\] The final activation function is another structural parameter left to the designer of the network. The typical choices are Linear units for Gaussian output distributions Sigmoid units for Bernoulli output distributions Softmax units for Multinoulli output distributions (Goodfellow et al., 2016) The learning rate \\(\\omega\\) and a cost function are meta-parameters, given by the user creating the neural network. These two parameters are used during the training of the network. During training, gradient descent is used to descend the cost function in order to find the optimal parameters for the network. Figure 2.6: https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471 Can improve this image and example, but the lesson is good. Figure 2.6 We can see that the loss, perhaps MSE for an example, is a function of \\(m\\) and \\(c\\) which is minimized at \\(m = 0\\) and \\(c = 0\\). The learning rate is the amount the functions coefficients are updated as the cost function is optimized using gradient descent. The training of the neural network aims to drive the approximating function to the underlying function. This is done with gradient learning using a loss function, or maximum likelihood estimation. Most modern neural networks are trained using maximum likelihood, in which the cost function is the negative log-likelihood between the training data and model distribution (Goodfellow et al., 2016). Loss functions are ways of describing performance of a model on predicting labels of a data set. The loss function takes the model and data as inputs, and outputs a real number. Loss functions can be descended in training by optimization to lead the network to find improvements in weights leading to more accurate predictions. Loss functions allow for another degree of customization in the training of a network, such as in Ridge and LASSO regression. These algorithms add a weighting to the typical mean squared error loss function which penalizes the weights on predictors in polynomial regression. These methods introduce bias into otherwise unbiased algorithms, but reduce the variability of the model across different draws of data from the same distribution, aiming to reduce the real test loss and improve the model. The cross entropy between the data distribution and the model distribution is the typical choice (Goodfellow et al., 2016). Cross Entropy: \\[ \\int_\\chi P(x) \\log Q (x) dr(x) = E_p [- \\log Q] \\] Take for example the Mean Squared Error cost function: \\[ MSE = \\frac{1}{n} \\sum_{k=1}^n (y - \\hat y) \\] MSE, a typical loss function for regression applications, takes the mean squared difference of the predicted label \\(\\hat y\\) and the true label \\(y\\) of the observations. Successive layers of the network learn representation transformations of the data which lend themselves to increasingly accurate description by linear regression and activation performed by the final layer, called the output layer. This process of successive transformation learns meaningful representations on which the output layer can be most accurate. Since the full network does not have a single functional representation, it is nonparametric. The flexibility and power of neural networks in fields demanding domain knowledge is that they can approximate any function, per the Universal Approximation Theorem (Hornik et al., 1989; Cybenko, 1989), which states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired nonzero amount of error, provided that the network is given enough hidden units. Thus the only user decision required would be the structure of the network, solvable through validation set meta-analysis. 2.2.3 Representation Learning If you were handed a photograph and were asked if it contained car, there’s a good chance you would immediately know the answer. But how are you coming to this conclusion? The human mind recognizes patterns of the image it has seen before, like a round bumper, a rectangular windshield, and a few circular tires with a certain spatial relationship to come to the conclusion that there is, in fact, a car in the photograph. It is this process of representation learning that prompted early researchers [[Citation Needed]] to create representation-based learning algorithms to extrapolate on information in the same way as the human mind. The emulation of human neural cells was the birth of deep learning, a class of machine learning algorithms which takes its name from multiple layers of connected nodes simulating a proposed structure of the neurons of the human mind. Representation learning takes observation’s features as a first layer into a composition of functions, in which each layer (or function) transforms the data according to a learned representation transformation that the subsequent layer takes as an input. This composition allows for complex relationships of features to be learned in increasingly sophisticated ways, making neural networks ideal for large-dimensional datasets with complex predictor relationships. For large-dimensional features, such as in Consumer Spending Data, images, or audio, these hierarchical representations (or layered representations) are important for distilling human-like feature extraction and iterative predictor space transformation. To achieve hierarchical representation properties, subsequent layers understand more complex functions of input variables as each layer transforms inputs and optimizes weights and weight relationships that minimize the overall loss of the network. As each layer receives the transformed output of the previous layer, more complex relationships of features can be derived. Representation learning is extremely important to the broad promises of neural networks in practice. The basis for this strength is that subsequent layers of the network learn meaningful structures of the input data associated with a lower loss score. Correlations of predictors forming a functional relationship of features to output which induce loss function descent will be internalized by the composition of subsequent functions. This property of representation learning is significant for investigating the necessity of independent and identically distributed data in deep learning algorithms, as it could be the case that significance of the inclusion probability can be learned as a meaningful predictor, with no special tweaks or preprocessing necessary for the algorithm. No special tweeks required is extremely meaningful as it circumvents the necessity of incorporating domain knowledge and expertise in current imputation methods, which holds back expedient and lightweight research. These advantages of Hierarchical and Distributed Representation transformation give neural networks huge advantages in accuracy and fitting capability for data with a massive hypothesis space. A hypothesis space is the space of all possible answers to a question. Image classification, for instance, represents a hypothesis space of pixels with unknown correlations that must be trained with label relationships to determine the correct distillation of millions of pixels to a most-likely class label. Thus the curse of dimensionality common throughout machine learning is mitigated through this manifold learning process. Neural networks thrive in the interaction of many predictors, due to the nature of representation learning which excels in covariate relations and distilling information encoded between many features. Popular modern applications are image and waveform audio data, in which machine learning problems become dominated by the Curse of Dimensionality. This common machine learning problem arises when the amount of data is insignificant when compared to the hypothesis space or feature space, and there are sparse observations for some regions. Machine learning needs many observations with each combination of values, but this becomes quickly infeasible for data with thousands of features. The peaking phenomena dictates that there is an optimal number of features to describe the data before the curse of dimensionality creates problematic sparsity and dominating observations with few neighbors . Neural networks are known to resist this commonplace issue due to the distributed learning property, wherein each node is sensitive to only particular features. Distributed representation is a powerful implicit component of neural networks in which neurons divide feature space to better handle feature interactions: suppose an image-recognition system could recognize cars, trucks, and birds, as well as distinguish if these objects are red, green, or blue. One way of representing these inputs could be to have a separate neuron for each combination: red truck, red car, red bird, and so on for nine independent neurons. Distributed representation, however, could partition these workloads by having three neurons for color and three for object type. In addition to reducing the number of neurons required dimensionally, this also distributes the learning demands of each neuron. The neuron describing redness is able to learn about redness from any category, not one specific category as the red bird neuron must (Goodfellow et al., 2016). Neural networks approximate nonlinear functions by applying linear models not to the features x, but to a transformed input, \\(\\phi(x)\\), where \\(\\phi\\) is a nonlinear transformation. \\(\\phi\\) provides a new, more meaningful representation for \\(x\\). The question then is how to choose the mapping \\(\\phi\\): One option is to manually engineer \\(\\phi\\). This takes a huge speciality of domain knowledge and practitioner specialization, with little transfer between domains. This was the dominant method before deep learning (Goodfellow et al., 2016). The strategy of neural networks comes from the learning of \\(\\phi\\). “In this approach, we have a model y = f(x; theta, w) as specified in the neural network introduction. Due to the Universal Approximation Theorem, the nonparametric deep feedforward network can learn a functional approximation from the input to the desired output. This method sacrifices the training convexity of the other two, but benefits from the genericity of the specifications. The human user need only specify a general function family rather than exactly the correct function, but can still benefit from designing families of \\(\\phi(x; \\theta)\\) that they expect to be relevant (Goodfellow et al., 2016). The neural network approximates some true underlying function \\(f^*(p; \\theta)\\) of the predictors \\(x\\) to the output category, \\(y\\), and learns the coefficients \\(\\theta\\) of the series of linear transformations composing the layers that result in the best function approximation. The number of functions in the composition is the called the depth of the model. Our model is called \\(\\hat f\\), the generative function it seeks to approximate is called \\(f\\). The outputs of our model are \\(\\hat y\\) “guess at y” and the true labels are \\(y\\). 2.2.4 Neural Networks for Complex Survey Data From an optimists’ perspective, the need for data preprocessing or special conditions on the loss function training the model would be unnecessary: If learning the correlations and underlying distributions associated with rare observations from complex survey design would truly lower the network’s loss, it should be learned and accounted for without need to perform special external transformations on the data to “undo” the effects of complex sample design. For this reason, it is significant to compare the potentially superior results of a naive model to one with superfluous data transformations done. A neural network model with access to an informative \\(\\pi\\) feature ideally would approximate the function relating the inclusion probability and features to labels, without the need for extreme domain knowledge and manual feature engineering. Neural networks are extremely promising algorithms in supervised learning, and the weakness of inferability is not a problem for imputation tasks. The nonparametric nature of neural networks means that little domain knowledge is required, and no feature engineering is required, to have a neural network make meaningful label predictions using feature relationships. Nonparametric (circumvent domain knowledge) (per universal approximation thm) Resist curse of dimensionality (that other parametrics and nonparametrics are exposed to) Have this be mentioned at the end of chapter 2 and explored w/ notation as chapter 3 Our goal is to diagnose potential bias incurred when ignoring complex survey design on data during the usage of deep neural networks. We have three algorithm tweaks for accommodating the influence of complex survey design. "],
["3-methods.html", "Chapter 3 Methods 3.1 Mean Estimation Methods 3.2 Note", " Chapter 3 Methods Neural Network Imputation in Complex Survey Design: “In a complex survey design, characteristics of the population may affect the sample and are used as design variables. Sample design involves the concepts of stratification, clustering, etc. These concepts usually reflect a complex population structure and should be accounted for during the analysis. In design-based inference, the main source of random variation is induced by the sampling mechanism. Furthermore, in complex survey design, the variance is the average squared deviation of the estimate from its expected value, averages over all possible samples which could be obtained using a given design. Design-based approaches make use of sampling weights as part of the estimation and inference procedures” “One major advantage of artificial neural networks (ANN) is their flexibility in modelling many types of nonlinear relationships. ANNs can be structures to account for complex survey designs and for unit nonresponse as well. In general, sampling weights have been used to adjust for the complex sampling design using unequal sampling.” They use two methods: weighted least squares, and constructing the ANN according to sampling design. Missingness is an extremely common problem in real data sets. Many analyses and algorithms such as regression, classification, PCA, and clustering done throughout the sciences rely on having entirely complete observations. For this reason, some strategy must be adopted to transform a raw data set into an analyzable one. There are multiple approaches for a researcher interested in going about this process. The researcher has a data set gathered under a complex survey design where the features \\(x_1,.x_n\\) and respondent probability \\(\\pi_i\\) is known for all \\(i\\), though the label \\(y_i\\) may be missing due to item nonresponse. In order to create a data set with no missing values, the researcher may choose to adopt one of the following methods: 3.0.1 Drop.NA One option is to simply remove the observations with missing labels from the data set. This method is extremely easy to implement and serves as a sure-fire way to end up with a data set with no missing values. There are two downsides to this method: The first is that removing observations with missingness obviously decreases the size of the data set and can nontrivially reduce the fidelity of models aiming to understand the population from which the data was taken. The second problem is the assumption of random missingness. If there is any correlation of label to amount of missingness, systematic bias is introduced into the data set, as discussed in Chapter 1. If there is a possibility that larger values are more likely to be dropped, then as a result the sample mean would underestimate the population. 3.1 Mean Estimation Methods If the researcher is interested only in estimating population mean: A common statistic of interest to a researcher is an estimate of the population mean \\(\\mu_y\\) using the information from the complex sample with missingness, but taking the naive mean of complete cases is insufficient for two reasons. The first has been discussed in chapter 1, which is the potential of systematic missingness in the data. The second is that the naive mean makes the assumption that the observations represent equal proportions of the population, as in an i.i.d. sample. Naive Mean: \\[ \\hat \\mu_y = \\frac{1}{r} \\sum_{i=1}^r y_i \\] Where \\(r\\) are the complete-case respondents. We resolve this issue in the mean estimate formula by weighting contributions to the mean by \\(\\frac{1}{\\pi_i}\\), the approximate number of population members represented by observation \\(i\\): Pi-Corrected Naive Mean: \\[ \\hat \\mu_y = \\frac{1}{r} \\sum_{i=1}^r y_i \\frac{1}{\\pi_i} \\] This resolves the problem of ignoring the survey design in estimation, but does not account for systematic bias resulting from missingness. This will be an under-estimate of the true population mean in the presence of systematic missingess of large observations. Let \\(\\hat \\mu_y\\) be the sample estimate of the population mean \\(\\mu_y\\). The oracle mean can be considered the “best estimate” of \\(\\mu_y\\), but it is only available in simulation. The oracle mean uses information that the simulation manually drops so as to create an ideal population estimate given a survey sample: Oracle Mean: \\[ \\hat \\mu_y = \\frac{1}{n} \\sum_{i=1}^n y_i \\frac{1}{\\pi_i} \\] The oracle mean will be used as a benchmark against which the other methods will be compared. In order to combat the presence of systematic missing values, we utilize imputation to label the missing observations with the best estimate of \\(y_i\\). Imputation has the added benefit of approximating a complete-case dataset without dropping observations. Works for both: For model-imputation methods, they each use this special mean-predictor function. (the N_hat one) The following methods utilize different imputation techniques to create a complete-case dataset by estimating missing values based on information from the complete cases. Once the missing values are imputed, the population mean is estimated hat_N_sample &lt;- sum(1/df\\(pi) statistic_tracker\\)nn_deriv_imp_mean[i] &lt;- (1 / hat_N_sample)*(sum(reduced_df\\(y / reduced_df\\)pi) + sum(nn_y_hat / dropped_obs$pi)) Imputation Mean Estimator: \\[ \\hat \\mu_y(\\text{method}) = \\frac{1}{\\hat N} (\\sum_{i=1}^{r} \\frac{y_i}{\\pi_i} + \\sum_{i=1}^{y-r} \\frac{\\hat y_i}{\\pi_i}) \\] Where \\(y-r\\) are the missing cases, \\(r\\) are the respondents, and \\(\\hat N = \\sum_{i=1}^n \\frac{1}{\\pi_i}\\). 3.1.1 Median Imputation Median imputation is another easy way to get complete cases in data for analysis or estimation. Median imputation simply fills in the missing labels with the median of the respondent labels. Median imputation has multiple problems for analysis or estimation. The median offers equal weighting to all observations in a data set, meaning it destroys the informativity of the inclusion probability \\(\\pi\\). It also removes correlation of feature and label, making analyses such as PCA less informative, as covariate relations dictate axis derivations. Median imputation is extremely fast to execute and implement, but creates noninformative observations in the same manner as the Drop.NA method. 3.1.2 Linear Regression Imputation (weighted pi) Linear regression is a convex optimization problem of \\(n+1\\) parameters, where \\(\\hat f(x) = \\hat y = (m_1x_1 + .+ m_nx_n)+b\\) is the estimate of \\(y\\) for observation \\(i\\). Using the mean of the squared difference between the predicted and actual responses of a training data set, Weighted-MSE Linear Regression scales the squared error contribution of each observation by \\(\\frac{1}{\\pi_i}\\) to account for rare-case observations with potentially systematic missingess: \\[ \\text{MSE}(f) = \\frac{1}{r} \\sum_{i=1}^r (\\frac{1}{\\pi}(\\hat{y} - y))^2 \\] 3.1.3 Naive Neural Network Imputation Neural Network Imputation is our baseline model for estimating a population mean. The number of hidden layers, nodes, and loss is left to the user, which would be an incorporation of domain knowledge or exploratory modelling to derive a reasonable model load for learning the data’s generative function. In the context of the researcher having minimal knowledge of the data, a neural network with 2 hidden layers of 64 and 32 units respectively activated by relu functions is a reasonable starting point. Overtraining due to overflexible models can be stimied with a validation set, assuming the data is not small (\\(n &gt; 10^3\\)). “Naive” neural network imputation refers to this model not having access to the \\(\\pi\\) feature of the observations as a predictor or incorporate it in any way. This model uses the assumption that the data is i.i.d as a representative for ignoring the survey design, an assumption which is known to be a significant problem. Regardless, the neural network should approximate the generative function with some nonparametric fit, but underestimate the population mean as a result of systematic missingness and observation equity. 3.1.4 Loss-Weighted Neural Network Imputation Loss-Weighted Neural Network Imputation takes inspiration from the weighted linear regression algorithm. This neural network training uses the same \\(\\pi\\)-weighted MSE, but sacrifices the convexity of the loss function, which means pervasive local minima which are distracting to the learning algorithm. The existence of local minima from the high-dimensional model space comes from the flexibility of the many hidden neurons weight transformations within the model. A loss-weighted neural network hopes to account for systematic missingness in the data by heavily punishing the loss term generated from rarer observations. Since rare observations are more likely to be missing, they must be given more weight since they appear less in the training data then the population. Thus a weighting scheme attempts to un-do systematic missingness by making the rarer observations as “heavy” as they would be in the true population by making outliers be worth multiple observations to the loss contribution. 3.1.5 \\(\\pi\\)-Feature Neural Network Imputation A \\(\\pi\\)-feature neural network has access to \\(\\pi\\) as a predictor during training and testing. This is a realistic assumption to make as data collected under a complex survey design must have a \\(\\pi\\) probability regardless of whether the label is missing. This method has the benefit of adapting to whether \\(\\pi\\) is truly correlated to \\(y\\), which the loss-weighted method assumes. A neural network optimist could claim that if there is a significant relationship of \\(\\pi\\) to \\(y\\), it will be reflected in the loss during training and the network will adapt accordingly to the information provided by the feature. However if \\(\\pi\\) and \\(y\\) are uncorrelated and the missingess is random, the network will not still weight the observations and will correctly ignore the feature to create more accurate predictions with no need for domain knowledge on the relationship of the missingness. include_graphics() #“C:DataWriting Rmd.png” # but one of the inputs is also pi 3.1.6 Weighted Resample Neural Network Imputation The weighted resample method uses the same model as the naive neural network imputation but uses a data preprocessing step to incorporate the survey design information. A weighted resample of size \\(n\\) is taken from the sample with replacement with observation selected by probability \\(\\frac{1}{\\pi}\\). The inference of this method is an attempt to “undo” the effects of survey design in gathering the data. A weighted resample in which observations are selected by the inverse inclusion probability uses the insight that an observation with inclusion probability \\(\\pi\\) represents \\(\\frac{1}{\\pi}\\) members of the population. By sampling on this probability, the resampled data set should be an estimate of an i.i.d. sample from the population, making it viable for supervised learning ignoring the survey design elements. From this point the naive neural network method is applied (and directly compared to naive neural network estimates), since ideally this is i.i.d data without need for survey design information tweaks on the algorithm. 3.1.7 Derived Feature Neural Network Imputation This method pre-processes the data by deriving additional features, such as the product of two features and the product of a feature and the inclusion probability. The intention of this method is expediting the training process to allow the approximation of more complex generative functions with less model capacity and training. Intuitively, the relationship of \\(\\pi\\) to \\(x_i\\) in the missing data, should it be relevant, will be more easily learned for superior prediction accuracy. 3.2 Note You can add a Reed Thesis citation1 option. The best way to do this is to use the phdthesis type of citation, and use the optional “type” field to enter “Reed thesis” or “Undergraduate thesis.” Noble (2002)↩ "],
["4-simulation.html", "Chapter 4 Simulation 4.1 Exploration of Methods in Simulation 4.2 High-Dimension Simulation 4.3 Monte Carlo Simulation 4.4 Creating a Simulated Population", " Chapter 4 Simulation Just 4 needs more work - ask kelly what it should say? Currently has mention of high-dimension, monte carlo, and methods (needs expansion) Discussion of outcomes Discussion of setups: generative function, number noisy variables, noise, results, all the parameters that i’ve chosen thus far Could someone re-create this simulation with the descriptions I gave them? Do everything in Latex, not code () 4.1 Exploration of Methods in Simulation Simulated data is used to evaluate the methods in order to get an understanding of their performance on a data set with known characteristics. Full insight into the feature distributions, generative function, random noise, and systematic missingness allow for controlled experimentation on when certain methods thrive. Simulation allows for parameters such as label noise and feature informativity to be changed and measure the response of the methods across any domain. 4.2 High-Dimension Simulation Often in real-world data, there might be a large number of features, not all of which are necessarily correlated to the response label. For example in the Consumer Expenditure data age, gender, ethnicity, and education might have a predictive relationship with income, while a number of others such as XXXXXXX do not. The ability for a model to discern which features are relevant and which are not is a significant benefit both for inference and predictive consistency. Inspired by research in best feature selection methods, non-correlated features are used in the model evaluation step in an attempt to more realistically simulate complex data (Hastie, Tibshirani, &amp; Tibshirani, 2017). The addition of these features contributes to the curse of dimensionality and variability of estimates. These additional “noisy” parameters are simulated by making the generative function of the labels \\(y\\) not a function of some of the features. All methods used in the simulation are accompanied by their “oracle” counterparts, meaning the same method is run two times: one with access to all features, the oracle restricted only to the relevant features which are inputs to the generative function. 4.3 Monte Carlo Simulation Monte Carlo (MC) simulation attempts to overcome the inherent randomness of sampling and model training by repeated iterations of the sampling, training, and evaluation steps. A distribution of output estimates is made over many potential samples from the population to get a more complete interpretation of the results. It should be noted that neural networks can be more optimized than their performance in MC simulation. This is because a network can be intentionally over-trained on the training data, then re-trained from scratch to the number of epochs which minimized the validation loss. This poses a challenge in MC simulation, however, where many iterations without user input are needed. Additionally, runtime becomes a vastly greater concern when networks must be over-trained through many epochs, more than doubling the execution time. 4.4 Creating a Simulated Population The size of the population is \\(N=10^5\\) and the sample is \\(n=10^3\\) observations. Despite neural networks thriving in higher-dimension settings (both of features and observations), complex surveys are often implemented on real data with scarce observations, making a lower-size simulation more suitable for generalizability. The features simulated population is made as the first step of the process. Two informative features \\(p_1, p_2\\) are drawn from random uniform distribution: \\[ f(x) = \\frac{1}{30+30} \\text{ for } x \\in (-30, 30) \\] In addition to two informative features, there are \\(20\\) uninformative features drawn from the normal distribution. This distribution is arbitrary but can have unintended consequences due to accidental correlation, noted in {Hastie et al. (2017)}. The population labels \\(y\\) are a nonlinear function of the informative features: \\[ y = p_1^2 + p_2^2 \\] The labels are then made nonpolynomial by contorting the parabola into a “bowl with flat lip”: \\[ y&#39; = \\left\\{ \\begin{array}{cc} y &amp; \\hspace{5mm} y&lt;\\bar{y} \\\\ \\bar{y} &amp; \\hspace{5mm} y \\geq \\bar{y} \\\\ \\end{array} \\right. \\] Random noise \\(\\epsilon\\) with \\(\\sigma = 1\\) is then added to the transformed labels: \\[ y&#39;&#39; = y&#39; + \\epsilon_y \\] An inclusion probability \\(pi\\) is then assigned to each observation with a strong correlation to \\(y\\), as well as random normal noise with \\(\\sigma = 1\\): \\[ \\pi = \\sqrt{y&#39;&#39;} + \\epsilon_\\pi \\] These values are then rescaled to (0,1) AND THEN SOMETHING ELSE NOT REALLY SURE WHAT MY CODE SAYS Combining the information \\(y,\\pi, p_1, p_2\\) and uncorrelated features gives the population on which the simulation will be performed. The true mean of the population labels \\(\\mu_y\\) is now known and recorded. The Monte Carlo simulation is performed for 100 iterations. This is a low number of trials, but is constrained by the computation intensity of running 10 neural networks trained for hundreds of iterations each, as well as performing many weighted samples. Within each iteration, a single estimation procedure is performed. First, a weighted sample from the population is taken with respect to the inclusion probabilities \\(\\pi_i\\) on each observation. From this sample of \\(n=10^3\\) observations, an oracle data set is made by dropping the noise columns, keeping only \\(p_1, p_2, \\pi,\\) and \\(y\\). This data set will be used as a benchmark to mirror each imputation method in order to gauge the potentially harmful effects of uninformative features common to real data. The first step is to record the oracle mean, calculated using information the other methods are not privy to: \\[ oracle mean equation here instead of chapter 3? \\] All subsequent methods are “real” methods, to be performed where there is missingness in the data. \\(20\\%\\) of the observations will havea missing label. Labels are dropped from the sample data set weighted by \\(y\\), so large labels are more likely to be absent. The result is the form of data we are using to simulate method performance: a complex sample taken from a population with some kind of systematic bias in item response. The \\(\\pi\\)-corrected mean estimate, weighted linear regression, and median imputation methods are all described fully in chapter 3, and are closed-form algorithms with no randomness in the fitting and estimation processes. The neural network models, however, have a much larger degree of flexibility and randomness involved in the implementation and execution of the estimate during simulation. Neural networks are trained using normalized data on non-convex loss curves in high-dimensional space, making their optimization process stochastic. This simulation uses the heuristic adam optimizer. Each model in this simulation uses a 2 hidden layer network with 32 hidden units per layer. Each layer is wrapped in the relu activation function: \\[ \\text{relu}(x) = \\max(0,x) \\] The breadth and depth used here is a heuristic size which could potentially be optimized to suit the problem. This, however, creates a conflict of interest with information leakage which the other methods are not privy to, which induces bias for the success of these algorithms. To maintain the narrative of a minimal-knowledge naive fit, these models use a reasonable exploratory capacity. The models in the fitting process are manually over-trained, then re-trained from scratch to an optimal state. This is done using a validation data set of \\(15\\%\\) of the training data. Once trained to an approximate loss minima, the models predict on holdout test data (the observations missing labels) and the population mean estimate is computed and recorded. "],
["conclusion.html", "Conclusion 4.5 Future Work", " Conclusion If we don’t want Conclusion to have a chapter number next to it, we can add the {-} attribute. More info And here’s some other random info: the first paragraph after a chapter title or section head shouldn’t be indented, because indents are to tell the reader that you’re starting a new paragraph. Since that’s obvious after a chapter or section title, proper typesetting doesn’t add an indent there. 4.5 Future Work There are so many exciting unexplored methods and tweaks that would be worth investigating. Many I don’t know how to implement, couldn’t find a good use, don’t understand enough, or was computationally restricted, especially during monte carlo simulation! Can do a paragraph on the most important things here: something better than linear regression: local polynomial, etc training neural networks correctly: finding the validation minima then re-training every time. I was prevented by computational intensity tweaks on neural networks: reinforcement learning, dropout layers, noise layers, pre-trained networks re-visiting weighted resampling by trying bootstrapping methods, bigger resample. bootstrapping many datasets, training on each, and taking mean of estimates. somethign like this to stabilize predictions and sampling variability. If you feel it necessary to include an appendix, it goes here. --> "],
["A-the-first-appendix.html", "A The First Appendix", " A The First Appendix This first appendix includes all of the R chunks of code that were hidden throughout the document (using the include = FALSE chunk tag) to help with readibility and/or setup. In the main Rmd file # This chunk ensures that the thesisdown package is # installed and loaded. This thesisdown package includes # the template files for the thesis. if(!require(devtools)) install.packages(&quot;devtools&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(thesisdown)) devtools::install_github(&quot;ismayc/thesisdown&quot;) library(thesisdown) In Chapter ??: "],
["B-the-second-appendix-for-fun.html", "B The Second Appendix, for Fun", " B The Second Appendix, for Fun "],
["references.html", "References", " References "]
]
