
# Methods

Neural Network Imputation in Complex Survey Design:
"In a complex survey design, characteristics of the population may affect the sample and are used as design variables. Sample design involves the concepts of stratification, clustering, etc. These concepts usually reflect a complex population structure and should be accounted for during the analysis. In design-based inference, the main source of random variation is induced by the sampling mechanism. Furthermore, in complex survey design, the variance is the average squared deviation of the estimate from its expected value, averages over all possible samples which could be obtained using a given design. Design-based approaches make use of sampling weights as part of the estimation and inference procedures"
"One major advantage of artificial neural networks (ANN) is their flexibility in modelling many types of nonlinear relationships. ANNs can be structures to account for complex survey designs and for unit nonresponse as well. In general, sampling weights have been used to adjust for the complex sampling design using unequal sampling." They use two methods: weighted least squares, and constructing the ANN according to sampling design.

Missingness is an extremely common problem in real data sets. Many analyses and algorithms such as regression, classification, PCA, and clustering done throughout the sciences rely on having entirely complete observations. For this reason, some strategy must be adopted to transform a raw data set into an analyzable one.
There are multiple approaches for a researcher interested in going about this process. The researcher has a data set gathered under a complex survey design where the features $x_1,.x_n$ and respondent probability $\pi_i$ is known for all $i$, though the label $y_i$ may be missing due to item nonresponse.

In order to create a data set with no missing values, the researcher may choose to adopt one of the following methods:

### Drop.NA
One option is to simply remove the observations with missing labels from the data set. This method is extremely easy to implement and serves as a sure-fire way to end up with a data set with no missing values. There are two downsides to this method: The first is that removing observations with missingness obviously decreases the size of the data set and can nontrivially reduce the fidelity of models aiming to understand the population from which the data was taken. The second problem is the assumption of random missingness. If there is any correlation of label to amount of missingness, systematic bias is introduced into the data set, as discussed in Chapter 1. If there is a possibility that larger values are more likely to be dropped, then as a result the sample mean would underestimate the population.

## Mean Estimation Methods

If the researcher is interested only in estimating population mean:

A common statistic of interest to a researcher is an estimate of the population mean $\mu_y$ using the information from the complex sample with missingness, but taking the naive mean of complete cases is insufficient for two reasons. The first has been discussed in chapter 1, which is the potential of systematic missingness in the data. The second is that the naive mean makes the assumption that the observations represent equal proportions of the population, as in an *i.i.d.* sample.

Naive Mean:
$$
\hat \mu_y = \frac{1}{r} \sum_{i=1}^r y_i
$$
Where $r$ are the complete-case respondents.

We resolve this issue in the mean estimate formula by weighting contributions to the mean by $\frac{1}{\pi_i}$, the approximate number of population members represented by observation $i$:

Pi-Corrected Naive Mean:
$$
\hat \mu_y = \frac{1}{r} \sum_{i=1}^r y_i \frac{1}{\pi_i}
$$
This resolves the problem of ignoring the survey design in estimation, but does not account for systematic bias resulting from missingness. This will be an under-estimate of the true population mean in the presence of systematic missingess of large observations.

Let $\hat \mu_y$ be the sample estimate of the population mean $\mu_y$. The oracle mean can be considered the "best estimate" of $\mu_y$, but it is only available in simulation. The oracle mean uses information that the simulation manually drops so as to create an ideal population estimate given a survey sample:

Oracle Mean:
$$
\hat \mu_y = \frac{1}{n} \sum_{i=1}^n y_i \frac{1}{\pi_i}
$$
The oracle mean will be used as a benchmark against which the other methods will be compared.

In order to combat the presence of systematic missing values, we utilize imputation to label the missing observations with the best estimate of $y_i$. Imputation has the added benefit of approximating a complete-case dataset without dropping observations.

Works for both:
For model-imputation methods, they each use this special mean-predictor function. (the  N_hat one)

The following methods utilize different imputation techniques to create a complete-case dataset by estimating missing values based on information from the complete cases. Once the missing values are imputed, the population mean is estimated

hat_N_sample <- sum(1/df$pi)
  statistic_tracker$nn_deriv_imp_mean[i] <- (1 / hat_N_sample)*(sum(reduced_df$y / reduced_df$pi) 
                                                                + sum(nn_y_hat / dropped_obs$pi))

Imputation Mean Estimator:
$$
\hat \mu_y(\text{method}) = \frac{1}{\hat N} (\sum_{i=1}^{r} \frac{y_i}{\pi_i} + \sum_{i=1}^{y-r} \frac{\hat y_i}{\pi_i})
$$
Where $y-r$ are the missing cases, $r$ are the respondents, and $\hat N = \sum_{i=1}^n \frac{1}{\pi_i}$.

### Median Imputation
Median imputation is another easy way to get complete cases in data for analysis or estimation. Median imputation simply fills in the missing labels with the median of the respondent labels. Median imputation has multiple problems for analysis or estimation. The median offers equal weighting to all observations in a data set, meaning it destroys the informativity of the inclusion probability $\pi$. It also removes correlation of feature and label, making analyses such as PCA less informative, as covariate relations dictate axis derivations. Median imputation is extremely fast to execute and implement, but creates noninformative observations in the same manner as the Drop.NA method.

### Linear Regression Imputation (weighted pi)
Linear regression is a convex optimization problem of $n+1$ parameters, where $\hat f(x) = \hat y = (m_1x_1 + .+ m_nx_n)+b$ is the estimate of $y$ for observation $i$. Using the mean of the squared difference between the predicted and actual responses of a training data set, Weighted-MSE Linear Regression scales the squared error contribution of each observation by $\frac{1}{\pi_i}$ to account for rare-case observations with potentially systematic missingess:
$$
\text{MSE}(f) = \frac{1}{r} \sum_{i=1}^r  (\frac{1}{\pi}(\hat{y} - y))^2
$$

### Naive Neural Network Imputation
Neural Network Imputation is our baseline model for estimating a population mean. The number of hidden layers, nodes, and loss is left to the user, which would be an incorporation of domain knowledge or exploratory modelling to derive a reasonable model load for learning the data's generative function. In the context of the researcher having minimal knowledge of the data, a neural network with 2 hidden layers of 64 and 32 units respectively activated by `relu` functions is a reasonable starting point. Overtraining due to overflexible models can be stimied with a validation set, assuming the data is not small ($n > 10^3$). "Naive" neural network imputation refers to this model not having access to the $\pi$ feature of the observations as a predictor or incorporate it in any way. This model uses the assumption that the data is *i.i.d* as a representative for ignoring the survey design, an assumption which is known to be a significant problem. Regardless, the neural network should approximate the generative function with some nonparametric fit, but underestimate the population mean as a result of systematic missingness and observation equity.

### Loss-Weighted Neural Network Imputation
Loss-Weighted Neural Network Imputation takes inspiration from the weighted linear regression algorithm. This neural network training uses the same $\pi$-weighted MSE, but sacrifices the convexity of the loss function, which means pervasive local minima which are distracting to the learning algorithm. The existence of local minima from the high-dimensional model space comes from the flexibility of the many hidden neurons weight transformations within the model. A loss-weighted neural network hopes to account for systematic missingness in the data by heavily punishing the loss term generated from rarer observations. Since rare observations are more likely to be missing, they must be given more weight since they appear less in the training data then the population. Thus a weighting scheme attempts to un-do systematic missingness by making the rarer observations as "heavy" as they would be in the true population by making outliers be worth multiple observations to the loss contribution.

### $\pi$-Feature Neural Network Imputation
A $\pi$-feature neural network has access to $\pi$ as a predictor during training and testing. This is a realistic assumption to make as data collected under a complex survey design must have a $\pi$ probability regardless of whether the label is missing. This method has the benefit of adapting to whether $\pi$ is truly correlated to $y$, which the loss-weighted method assumes. A neural network optimist could claim that if there is a significant relationship of $\pi$ to $y$, it will be reflected in the loss during training and the network will adapt accordingly to the information provided by the feature. However if $\pi$ and $y$ are uncorrelated and the missingess is random, the network will not still weight the observations and will correctly ignore the feature to create more accurate predictions with no need for domain knowledge on the relationship of the missingness.

```{r}
include_graphics()
#"C:\Users\Alexander\Documents\Thesis Data\Thesis Writing Rmd\AMMthesis\figure\network.png"
# but one of the inputs is also pi
```

### Weighted Resample Neural Network Imputation
The weighted resample method uses the same model as the naive neural network imputation but uses a data preprocessing step to incorporate the survey design information. A weighted resample of size $n$ is taken from the sample with replacement with observation selected by probability $\frac{1}{\pi}$. The inference of this method is an attempt to "undo" the effects of survey design in gathering the data. A weighted resample in which observations are selected by the inverse inclusion probability uses the insight that an observation with inclusion probability $\pi$ represents $\frac{1}{\pi}$ members of the population. By sampling on this probability, the resampled data set should be an estimate of an *i.i.d.* sample from the population, making it viable for supervised learning ignoring the survey design elements. From this point the naive neural network method is applied (and directly compared to naive neural network estimates), since ideally this is *i.i.d* data without need for survey design information tweaks on the algorithm.

### Derived Feature Neural Network Imputation

This method pre-processes the data by deriving additional features, such as the product of two features and the product of a feature and the inclusion probability. The intention of this method is expediting the training process to allow the approximation of more complex generative functions with less model capacity and training. Intuitively, the relationship of $\pi$ to $x_i$ in the missing data, should it be relevant, will be more easily learned for superior prediction accuracy.









# Tables, Graphics, References, and Labels {#ref-labels}

## Tables

## Bibliographies

Of course you will need to cite things, and you will probably accumulate an armful of sources. There are a variety of tools available for creating a bibliography database (stored with the .bib extension).  In addition to BibTeX suggested below, you may want to consider using the free and easy-to-use tool called Zotero.  The Reed librarians have created Zotero documentation at <http://libguides.reed.edu/citation/zotero>.  In addition, a tutorial is available from Middlebury College at <http://sites.middlebury.edu/zoteromiddlebury/>.

_R Markdown_ uses _pandoc_ (<http://pandoc.org/>) to build its bibliographies.  One nice caveat of this is that you won't have to do a second compile to load in references as standard LaTeX requires. To cite references in your thesis (after creating your bibliography database), place the reference name inside square brackets and precede it by the "at" symbol.  For example, here's a reference to a book about worrying:  [@Molina1994].  This `Molina1994` entry appears in a file called `thesis.bib` in the `bib` folder.  This bibliography database file was created by a program called BibTeX.  You can call this file something else if you like (look at the YAML header in the main .Rmd file) and, by default, is to placed in the `bib` folder.  


If you look at the YAML header at the top of the main .Rmd file you can see that we can specify the style of the bibliography by referencing the appropriate csl file.  You can download a variety of different style files at <https://www.zotero.org/styles>.  Make sure to download the file into the csl folder.

**Tips for Bibliographies**

- Like with thesis formatting, the sooner you start compiling your bibliography for something as large as thesis, the better. Typing in source after source is mind-numbing enough; do you really want to do it for hours on end in late April? Think of it as procrastination.
- The cite key (a citation's label) needs to be unique from the other entries.
- When you have more than one author or editor, you need to separate each author's name by the word "and" e.g. `Author = {Noble, Sam and Youngberg, Jessica},`.
- Bibliographies made using BibTeX (whether manually or using a manager) accept LaTeX markup, so you can italicize and add symbols as necessary.
- To force capitalization in an article title or where all lowercase is generally used, bracket the capital letter in curly braces.
- You can add a Reed Thesis citation^[@noble2002] option. The best way to do this is to use the phdthesis type of citation, and use the optional "type" field to enter "Reed thesis" or "Undergraduate thesis." 

