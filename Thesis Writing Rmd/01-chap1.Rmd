<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Complex Surveys

## Survey Statistics

> Researchers in the social science and health sciences are increasingly interested in using data from complex surveys to conduct the same sorts of analyses that they traditionally conduct with more straightforward data. Medical researchers are also increasingly aware of the advantages of well-designed subsamples when measuring novel, expensive variables on an existing cohort. - Lumley 1969

The implicit pervasiveness of survey statistics in all data motivates our exploration into its significance in imputation.

Survey statistics differ from statistics modelling in the specification of the random process that generates the data. In model-based statistics, some underlying generative model from which observations are drawn is assumed to exist. By understanding or approximating this model from data, one may draw conclusions on the nature of the generative function provided no meaningful changes to the data are made.

Contrary to model-based statistics, the analysis of complex survey samples are design based. The observations from a researcher-specified population have fixed features, and randomness is introduced when these observations are drawn from the population according to some stochastic design. This random process is under the control of the researcher, and can be known precisely. The significance of design-based methods is that the probability sample is the procedure for taking samples from a population, not just the resulting data. Using this method, the features of the population from which the observations are drawn may be estimated, but these conclusions may not generalize to other populations. With understanding of the survey design from which data observations arise, the researcher may make improved estimates of the population of study (Lumley 1969).

The probability sample is the fundamental concept of design-based inference. Taking a random sample of 36,000 people from Oregon is an example of a survey design which implies independent and equal probability sampling of all humans in the state. The Law of Large Numbers is invoked to assume the distribution of sampled observations represent the population from which they are drawn according to any features of interest to the researcher, such as height, weight, or age.

This type of surveying can be complicated by adding a stratifying element, such as randomly sampling 1,000 people from each of the 36 counties of Oregon. The data created by such a design would likely not be representative of the state, since people from lower-population counties would be more likely to be sampled. However, since the probability of each person in the sample being randomly selected is known (since the population of each county is known), this is still a probability sample. The key point of this process is that a probability sample is the procedure for taking samples from a population, not "just the data we happen to end up with" (Lumley 1969).

There are four requirements for a data set to be a probability sample:

1. Every individual in the population must have a non-zero probability of ending up in the sample.

2. The probability of inclusion must be known for every individual who does end up in the sample.

3. Every pair of individuals in the sample must have a non-zero probability of both ending up in the sample.

4. The probability of every pair of individuals being included in the sample must be known for every pair of individuals in the sample.

The fundamental statistical idea behind all of design-based inference is that an individual sampled with a sampling probability $pi_i$ represents $\frac{1}{pi_i}$ individuals in the population. The value $\frac{1}{pi_i}$ is called the sampling weight (Lumley 1969).

1 and 2 are necessary in order to get valid population estimates, 3 and 4 are necessary to work out the accuracy of the estimates. If individuals were sampled independently of each other the first two properties would guarantee the last two (Lumley 1969). Though 3 and 4 are requirements of a probability sample, they are often not included in datasets as they require an nxn matrix of probabilities, where n is the number of observations in the data set.

Data collected under a complex survey design have an additional layer of complexity and are not to be treated as typical independent and identically distributed (*i.i.d.*) data. Ignoring this complex survey design is found to create significant error in data analyses (Toth and Eltinge 2012). This concern motivates our exploration of accounting for survey design in neural network imputation.

## Imputation

Often in real-world data, there is some degree of missingness. This can be for any number of reasons, illustrated in Table 1 below.

```{r missingness, echo=FALSE, cache=TRUE}
## Make table
library(kableExtra)
library(dplyr)

df<- cbind(1:5, 1:5)

colnames(df) <- c("Symbol", "Description")
rownames(df) <- c("Features", "Label", "Label Estimate", "Inclusion Probability", "Population Mean")

df[,1] <- c("x_i", "y_i", "y_i hat", "pi_i", "mu_y")
df[,2] <- c("The set of observations feature responses", "Response label", 
            "Missing label estimate", "Survey-design inclusion probability", "True population mean")

print <- kable(df)
```

```{r missingness, echo=FALSE, cache=TRUE}
## Make table
library(kableExtra)
library(dplyr)

df<- cbind(1:3)

colnames(df) <- c("Description")
rownames(df) <- c("Noncoverage", "Total Nonresponse", "Item Nonresponse")

df[,1] <- c("An element in the target population is not included in the survey sampling frame", "A sampled element does not participate in the survey", "A responding sampled element fails to provide acceptable responses to one or more of the survey items")

knitr::kable(df, caption = "Types of missingess")
```
Table: Types of missingness
"Item nonresponse, which occurs when a sampled element participates in a survey but fails to provide acceptable responses to one or more of the survey items. Item nonresponse may arise because a respondent refuses to answer an item on the grounds that it is too sensitive, does not know the answer to the item, gives an answer that is inconsistent with answers to other items and hence is deleted in editing, or because the interviewer fails to ask the question or record the answer. The usual form of item nonresponse is imputation, which involves assigning a value for the missing response." 216 (Brick and Kalton 1996)

In many statistical analyses, observations with any degree of missingness cannot be present. For example, how would one perform a linear regression with observations that have features but no label? Dropping these terms would remove potentially huge swathes of the data set, particularly in multivariate data sets, and would create systematic bias. Suppose for example a data set with information on roses had a feature with stem length and a label on flower size. NA (not available) values for flower size might not be randomly distributed: there could, for example, be a systematic picking of large-flowered roses by passersby. To ignore these observations would lead the analyst to draw false conclusions on the relationship of stem length to flower size, and the distribution of flower sizes in the population. Consider Example 1:
```{r systematic}
# Example 1: what happens when systematic bias is ignored, and missing values are dropped
library(ggplot2)
library(dplyr)

tx <- seq(0, 8)
ty <- exp(tx)
plot(tx,ty)

df <- as_data_frame(cbind(tx, ty))

x <- tx[1:6]
y <- ty[1:6]
lin <- lm(y~x, data = df)

ggplot(data = df[1:6,], aes(x=tx, y=ty)) + 
  geom_point(color = 'blue') + 
  geom_abline(slope = lin$coefficients[2], intercept = lin$coefficients[1])

ggplot(data = df, aes(x=tx, y=ty)) + 
  geom_point(color = 'blue') + 
  geom_abline(slope = lin$coefficients[2], intercept = lin$coefficients[1])
```

Imputation attempts to address this common dilemma in real-world data. Imputation is the process of replacing missingness in data sets with some value that redeems the observation for some degree of analysis. "The aim of these methods is to compensate for the missing data in such a manner that the analysis file may be subjected to any form of analysis without the need for further consideration of the missing data" (Brick and Kalton 1996). Imputation assigns values to missing responses, which allows records with missingness to be retained during analysis. Ideally, imputation would eliminate bias in survey estimates caused by ignoring records with missing data. The catch is that imputation can destroy intervariable relationships, and by nature data is fabricated in the process which leads to overestimation of the precision of survey estimates.

There are stochastic and deterministic methods for imputation. Deterministic regression imputation is the predicted value from a regression trained on complete-case data. Stochastic imputation differs due to an additional residual added term to the predicted value, taken either from a random respondent or comparable observation, and is usually preferred due to the importance of shape parameters in many analyses (Brick and Kalton 1996).

One traditional method of imputation is the "Hot-Deck Method". This method was generally favorable when computation was at an expense, and required extensive knowledge of the survey variables in order to optimize, as explicit model-based imputation needs a valid model for every survey variable (Mairi and Miller, 2008). This thesis proposes naive artificial neural networks as a solution which requires minimal domain knowledge and resists the curse of dimensionality which other nonparametric methods are susceptible to, such as local polynomial regression (Maiti and Miller, 2008).